{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 5 - Loading and Saving Data in Spark.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dalgual/bigdai.github.io/blob/main/Chapter_5_Loading_and_Saving_Data_in_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD1zvV9JtRsN"
      },
      "source": [
        "# Loading and Saving Data in Spark\n",
        "### Revised by Jongwook Woo (jwoo5@calstatela.edu), 09/09/2021\n",
        "Revised for the latest Spark setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXcc49lmUYgz"
      },
      "source": [
        "Collab Only code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_U5TbtAUX_r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35001004-ece3-451e-f0d0-3419a865ea00"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# Jwoo: Error\n",
        "# !wget -q https://www-us.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "# !tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "# !pip install -q findspark\n",
        "\n",
        "# JWoo: as of 09/09/2021\n",
        "! rm -rf spark-3.1.2-bin-hadoop3.2.*\n",
        "#!wget -q https://www.apache.org/dyn/closer.lua/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!tar xzvf spark-3.1.2-bin-hadoop3.2.tgz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spark-3.1.2-bin-hadoop3.2/\n",
            "spark-3.1.2-bin-hadoop3.2/R/\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/sparkr.zip\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/worker/\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/worker/worker.R\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/worker/daemon.R\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/tests/\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/tests/testthat/\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/tests/testthat/test_basic.R\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/profile/\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/profile/shell.R\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/profile/general.R\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/doc/\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/doc/sparkr-vignettes.html\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/doc/sparkr-vignettes.Rmd\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/doc/sparkr-vignettes.R\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/doc/index.html\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/R/\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/R/SparkR\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/R/SparkR.rdx\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/R/SparkR.rdb\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/Meta/\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/Meta/features.rds\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/Meta/package.rds\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/Meta/nsInfo.rds\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/Meta/vignette.rds\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/Meta/Rd.rds\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/Meta/links.rds\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/Meta/hsearch.rds\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/DESCRIPTION\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/NAMESPACE\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/html/\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/html/R.css\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/html/00Index.html\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/INDEX\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/help/\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/help/aliases.rds\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/help/AnIndex\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/help/SparkR.rdx\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/help/SparkR.rdb\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/help/paths.rds\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/workers.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/stop-workers.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/stop-worker.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/stop-thriftserver.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/stop-slaves.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/stop-slave.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/stop-mesos-shuffle-service.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/stop-mesos-dispatcher.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/stop-master.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/stop-history-server.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/stop-all.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/start-workers.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/start-worker.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/start-thriftserver.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/start-slaves.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/start-slave.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/start-mesos-shuffle-service.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/start-mesos-dispatcher.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/start-master.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/start-history-server.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/start-all.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/spark-daemons.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/spark-daemon.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/slaves.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/decommission-worker.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/decommission-slave.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/spark-config.sh\n",
            "spark-3.1.2-bin-hadoop3.2/python/\n",
            "spark-3.1.2-bin-hadoop3.2/python/dist/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark.egg-info/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark.egg-info/SOURCES.txt\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark.egg-info/top_level.txt\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark.egg-info/requires.txt\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark.egg-info/dependency_links.txt\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark.egg-info/PKG-INFO\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/python/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/python/pyspark/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/python/pyspark/shell.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/__pycache__/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/__pycache__/install.cpython-38.pyc\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/evaluation.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/common.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/common.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/clustering.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/classification.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/_typing.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/util.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/util.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/tree.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/tree.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/tests/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/tests/test_util.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/tests/test_streaming_algorithms.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/tests/test_stat.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/tests/test_feature.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/tests/test_algorithms.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/tests/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/tests/test_linalg.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/regression.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/clustering.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/classification.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/stat/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/stat/_statistics.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/stat/__init__.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/stat/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/stat/KernelDensity.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/stat/KernelDensity.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/stat/test.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/stat/test.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/stat/distribution.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/stat/distribution.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/stat/_statistics.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/regression.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/recommendation.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/recommendation.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/random.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/random.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/linalg/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/linalg/distributed.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/linalg/__init__.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/linalg/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/linalg/distributed.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/fpm.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/fpm.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/feature.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/feature.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/evaluation.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/wrapper.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/wrapper.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/util.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/util.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tree.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tests/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tests/test_wrapper.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tests/test_util.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tests/test_stat.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tests/test_pipeline.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tests/test_persistence.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tests/test_linalg.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tests/test_image.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tests/test_feature.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tests/test_algorithms.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tests/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tests/test_tuning.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tests/test_training_summary.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tests/test_param.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tests/test_evaluation.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tests/test_base.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/stat.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/stat.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/regression.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/recommendation.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/pipeline.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/pipeline.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/param/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/param/shared.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/param/shared.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/param/_shared_params_code_gen.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/param/_shared_params_code_gen.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/param/__init__.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/param/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/linalg/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/linalg/__init__.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/linalg/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/image.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/image.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/functions.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/fpm.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/feature.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/evaluation.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/common.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/common.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/clustering.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/classification.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/base.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/_typing.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tuning.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tuning.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tree.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/regression.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/recommendation.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/functions.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/fpm.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/feature.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/evaluation.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/clustering.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/classification.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/base.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/join.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/java_gateway.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/install.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/find_spark_home.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/files.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/files.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/daemon.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/context.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/context.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/conf.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/conf.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/cloudpickle/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/cloudpickle/compat.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/cloudpickle/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/broadcast.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/broadcast.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/accumulators.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/accumulators.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/_typing.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/_globals.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/__init__.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/window.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/window.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/udf.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/types.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/types.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_utils.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_udf.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_types.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_streaming.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_session.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_serde.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_readwriter.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_udf_window.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_udf_typehints.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_udf_scalar.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_udf_grouped_agg.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_udf.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_map.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_cogrouped_map.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_datasources.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_context.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_conf.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_catalog.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_grouped_map.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_group.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_functions.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_dataframe.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_column.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_arrow.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/streaming.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/session.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/readwriter.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/utils.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/typehints.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/serializers.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/map_ops.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/map_ops.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/group_ops.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/group_ops.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/functions.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/functions.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/_typing/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/_typing/protocols/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/_typing/protocols/series.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/_typing/protocols/frame.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/_typing/protocols/__init__.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/_typing/__init__.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/types.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/group.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/group.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/context.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/conf.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/conf.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/column.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/catalog.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/avro/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/avro/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/avro/functions.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/avro/functions.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/_typing.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/__init__.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/udf.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/streaming.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/session.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/readwriter.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/functions.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/functions.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/context.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/column.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/catalog.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/shuffle.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/shell.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/serializers.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/resultiterable.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/resultiterable.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/resource/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/resource/tests/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/resource/tests/test_resources.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/resource/tests/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/resource/requests.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/resource/requests.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/resource/information.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/resource/information.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/resource/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/resource/profile.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/resource/profile.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/rddsampler.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/rdd.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/py.typed\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/profiler.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/profiler.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/worker.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/version.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/taskcontext.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/rdd.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/version.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/util.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/traceback_utils.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_worker.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_util.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_taskcontext.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_shuffle.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_serializers.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_readwrite.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_rddbarrier.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_rdd.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_profiler.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_pin_thread.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_join.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_install_spark.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_daemon.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_context.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_conf.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_broadcast.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_appsubmit.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/testing/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/testing/streamingutils.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/testing/sqlutils.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/testing/mlutils.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/testing/mllibutils.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/testing/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/testing/utils.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/taskcontext.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/util.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/tests/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/tests/test_listener.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/tests/test_kinesis.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/tests/test_dstream.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/tests/test_context.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/tests/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/listener.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/listener.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/kinesis.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/kinesis.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/dstream.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/context.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/context.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/dstream.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/storagelevel.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/storagelevel.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/status.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/status.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/statcounter.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/statcounter.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pylintrc\n",
            "spark-3.1.2-bin-hadoop3.2/python/lib/\n",
            "spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip\n",
            "spark-3.1.2-bin-hadoop3.2/python/lib/PY4J_LICENSE.txt\n",
            "spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/user_guide/\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/user_guide/python_packaging.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/user_guide/index.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/user_guide/arrow_pandas.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/reference/\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/reference/pyspark.streaming.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/reference/pyspark.ss.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/reference/pyspark.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/reference/pyspark.resource.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/reference/pyspark.mllib.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/reference/index.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/reference/pyspark.sql.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/reference/pyspark.ml.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/migration_guide/\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/migration_guide/pyspark_2.3_to_2.4.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/migration_guide/pyspark_2.3.0_to_2.3.1_above.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/migration_guide/pyspark_2.2_to_2.3.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/migration_guide/pyspark_1.4_to_1.5.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/migration_guide/pyspark_1.0_1.2_to_1.3.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/migration_guide/pyspark_2.4_to_3.0.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/migration_guide/index.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/getting_started/\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/getting_started/quickstart.ipynb\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/getting_started/index.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/getting_started/install.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/development/\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/development/testing.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/development/setting_ide.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/development/index.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/development/debugging.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/development/contributing.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/_templates/\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/_templates/autosummary/\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/_templates/autosummary/class_with_docs.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/_templates/autosummary/class.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/_static/\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/_static/css/\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/_static/css/pyspark.css\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/_static/copybutton.js\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/index.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/conf.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/make.bat\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/make2.bat\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/Makefile\n",
            "spark-3.1.2-bin-hadoop3.2/python/README.md\n",
            "spark-3.1.2-bin-hadoop3.2/python/MANIFEST.in\n",
            "spark-3.1.2-bin-hadoop3.2/python/.gitignore\n",
            "spark-3.1.2-bin-hadoop3.2/python/.coveragerc\n",
            "spark-3.1.2-bin-hadoop3.2/python/setup.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/run-tests.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/run-tests-with-coverage\n",
            "spark-3.1.2-bin-hadoop3.2/python/mypy.ini\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/userlibrary.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/userlib-0.1.zip\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/text-test.txt\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/streaming/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/streaming/text-test.txt\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/people_array_utf16le.json\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/people_array.json\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/people1.json\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/people.json\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=9/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/part-r-00007.gz.parquet\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/.part-r-00007.gz.parquet.crc\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/part-r-00005.gz.parquet\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/.part-r-00005.gz.parquet.crc\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00004.gz.parquet\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00002.gz.parquet\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00004.gz.parquet.crc\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00002.gz.parquet.crc\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2014/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2014/month=9/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/part-r-00008.gz.parquet\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/.part-r-00008.gz.parquet.crc\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/_metadata\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/_common_metadata\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/_SUCCESS\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=1/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=1/c=1/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=1/c=1/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=1/c=1/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=0/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=0/c=0/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=0/c=0/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=0/c=0/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/_SUCCESS\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/ages_newlines.csv\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/ages.csv\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/hello/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/hello/sub_hello/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/hello/sub_hello/sub_hello.txt\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/hello/hello.txt\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/SimpleHTTPServer.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_coverage/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_coverage/sitecustomize.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_coverage/coverage_daemon.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_coverage/conf/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_coverage/conf/spark-defaults.conf\n",
            "spark-3.1.2-bin-hadoop3.2/python/setup.cfg\n",
            "spark-3.1.2-bin-hadoop3.2/python/run-tests\n",
            "spark-3.1.2-bin-hadoop3.2/bin/\n",
            "spark-3.1.2-bin-hadoop3.2/bin/sparkR2.cmd\n",
            "spark-3.1.2-bin-hadoop3.2/bin/sparkR.cmd\n",
            "spark-3.1.2-bin-hadoop3.2/bin/sparkR\n",
            "spark-3.1.2-bin-hadoop3.2/bin/spark-submit2.cmd\n",
            "spark-3.1.2-bin-hadoop3.2/bin/spark-submit.cmd\n",
            "spark-3.1.2-bin-hadoop3.2/bin/spark-submit\n",
            "spark-3.1.2-bin-hadoop3.2/bin/spark-sql2.cmd\n",
            "spark-3.1.2-bin-hadoop3.2/bin/spark-sql.cmd\n",
            "spark-3.1.2-bin-hadoop3.2/bin/spark-sql\n",
            "spark-3.1.2-bin-hadoop3.2/bin/spark-shell2.cmd\n",
            "spark-3.1.2-bin-hadoop3.2/bin/spark-shell.cmd\n",
            "spark-3.1.2-bin-hadoop3.2/bin/spark-shell\n",
            "spark-3.1.2-bin-hadoop3.2/bin/spark-class2.cmd\n",
            "spark-3.1.2-bin-hadoop3.2/bin/spark-class.cmd\n",
            "spark-3.1.2-bin-hadoop3.2/bin/spark-class\n",
            "spark-3.1.2-bin-hadoop3.2/bin/run-example.cmd\n",
            "spark-3.1.2-bin-hadoop3.2/bin/run-example\n",
            "spark-3.1.2-bin-hadoop3.2/bin/pyspark.cmd\n",
            "spark-3.1.2-bin-hadoop3.2/bin/load-spark-env.sh\n",
            "spark-3.1.2-bin-hadoop3.2/bin/load-spark-env.cmd\n",
            "spark-3.1.2-bin-hadoop3.2/bin/find-spark-home.cmd\n",
            "spark-3.1.2-bin-hadoop3.2/bin/find-spark-home\n",
            "spark-3.1.2-bin-hadoop3.2/bin/docker-image-tool.sh\n",
            "spark-3.1.2-bin-hadoop3.2/bin/beeline.cmd\n",
            "spark-3.1.2-bin-hadoop3.2/bin/beeline\n",
            "spark-3.1.2-bin-hadoop3.2/bin/pyspark2.cmd\n",
            "spark-3.1.2-bin-hadoop3.2/bin/pyspark\n",
            "spark-3.1.2-bin-hadoop3.2/README.md\n",
            "spark-3.1.2-bin-hadoop3.2/conf/\n",
            "spark-3.1.2-bin-hadoop3.2/conf/workers.template\n",
            "spark-3.1.2-bin-hadoop3.2/conf/spark-env.sh.template\n",
            "spark-3.1.2-bin-hadoop3.2/conf/spark-defaults.conf.template\n",
            "spark-3.1.2-bin-hadoop3.2/conf/metrics.properties.template\n",
            "spark-3.1.2-bin-hadoop3.2/conf/log4j.properties.template\n",
            "spark-3.1.2-bin-hadoop3.2/conf/fairscheduler.xml.template\n",
            "spark-3.1.2-bin-hadoop3.2/data/\n",
            "spark-3.1.2-bin-hadoop3.2/data/streaming/\n",
            "spark-3.1.2-bin-hadoop3.2/data/streaming/AFINN-111.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/streaming_kmeans_data_test.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/sample_svm_data.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/sample_multiclass_classification_data.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/sample_movielens_data.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/sample_linear_regression_data.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/sample_libsvm_data.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/sample_lda_libsvm_data.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/sample_lda_data.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/sample_kmeans_data.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/sample_isotonic_regression_libsvm_data.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/sample_fpgrowth.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/sample_binary_classification_data.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/ridge-data/\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/ridge-data/lpsa.data\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/pic_data.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/pagerank_data.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/kmeans_data.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/iris_libsvm.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/partitioned/\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-02/\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-02/grayscale.jpg\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-02/chr30.4.184.jpg\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-01/\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-01/BGRA_alpha_60.png\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-01/BGRA.png\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-02/\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-02/DP802813.jpg\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-02/DP153539.jpg\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-02/54893.jpg\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-01/\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-01/not-image.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-01/29.5.a_b_EGDP022204.jpg\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/origin/\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/origin/multi-channel/\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/origin/multi-channel/grayscale.jpg\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/origin/multi-channel/chr30.4.184.jpg\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/origin/multi-channel/BGRA_alpha_60.png\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/origin/multi-channel/BGRA.png\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/origin/license.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/origin/kittens/\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/origin/kittens/not-image.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/origin/kittens/DP802813.jpg\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/origin/kittens/DP153539.jpg\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/origin/kittens/54893.jpg\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/origin/kittens/29.5.a_b_EGDP022204.jpg\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/license.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/gmm_data.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/als/\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/als/test.data\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/als/sample_movielens_ratings.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/graphx/\n",
            "spark-3.1.2-bin-hadoop3.2/data/graphx/users.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/graphx/followers.txt\n",
            "spark-3.1.2-bin-hadoop3.2/NOTICE\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-zstd.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-zstd-jni.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-xmlenc.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-vis-timeline.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-spire.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-sorttable.js.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-slf4j.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-scopt.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-scala.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-sbt-launch-lib.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-respond.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-reflectasm.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-re2j.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-pyrolite.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-py4j.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-protobuf.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-pmml-model.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-paranamer.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-netlib.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-mustache.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-modernizr.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-minlog.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-matchMedia-polyfill.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-machinist.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-leveldbjni.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-kryo.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-jsp-api.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-json-formatter.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-jquery.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-join.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-jodd.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-jline.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-jaxb-runtime.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-javolution.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-javax-transaction-transaction-api.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-javassist.html\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-janino.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-jakarta.xml.bind-api.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-jakarta.activation-api.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-jakarta-ws-rs-api\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-jakarta-annotation-api\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-istack-commons-runtime.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-graphlib-dot.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-f2j.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-dnsjava.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-datatables.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-dagre-d3.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-d3.min.js.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-cloudpickle.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-bootstrap.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-automaton.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-arpack.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-antlr.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-JTransforms.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-JLargeArrays.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-CC0.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-AnchorJS.txt\n",
            "spark-3.1.2-bin-hadoop3.2/LICENSE\n",
            "spark-3.1.2-bin-hadoop3.2/examples/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/PrefixSpanExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/PowerIterationClusteringExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/PMMLModelExportExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnSourceVectorExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnRowMatrixExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/NormalizerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/NaiveBayesExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/MultivariateSummarizer.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/MulticlassMetricsExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/MultiLabelMetricsExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/LogisticRegressionWithLBFGSExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/LatentDirichletAllocationExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/LDAExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/LBFGSExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/KernelDensityEstimationExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/KMeansExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/IsotonicRegressionExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingKolmogorovSmirnovTestExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingRegressionExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingClassificationExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostedTreesRunner.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/GaussianMixtureExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/FPGrowthExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/ElementwiseProductExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/DenseKMeans.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRunner.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRegressionExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeClassificationExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/CosineSimilarity.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/CorrelationsExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/Correlations.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/ChiSqSelectorExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/BisectingKMeansExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassificationMetricsExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassification.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/AssociationRulesExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/AbstractParams.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/Word2VecExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnySVD.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnyPCA.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/TFIDFExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SummaryStatisticsExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingTestExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLogisticRegression.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegressionExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StratifiedSamplingExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StandardScalerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SparseNaiveBayes.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SimpleFPGrowth.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SampledRDDs.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SVMWithSGDExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SVDExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/RankingMetricsExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/RandomRDDGeneration.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestRegressionExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestClassificationExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/VectorSlicerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/VectorSizeHintExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/VectorIndexerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/VarianceThresholdSelectorExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/UnivariateFeatureSelectorExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/UnaryTransformerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/TokenizerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/TfIdfExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/SummarizerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/StopWordsRemoverExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/StandardScalerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/SQLTransformerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/RobustScalerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestRegressorExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/RFormulaExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/QuantileDiscretizerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/PrefixSpanExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/PowerIterationClusteringExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/PolynomialExpansionExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/PCAExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/OneVsRestExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/OneHotEncoderExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/NormalizerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/NaiveBayesExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/NGramExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/MultilayerPerceptronClassifierExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/MulticlassLogisticRegressionWithElasticNetExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaTrainValidationSplitExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaCrossValidationExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/MinMaxScalerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/MaxAbsScalerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionWithElasticNetExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionSummaryExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LinearSVCExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionWithElasticNetExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LDAExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/KMeansExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/IsotonicRegressionExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/InteractionExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/IndexToStringExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ImputerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeRegressorExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeClassifierExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/GeneralizedLinearRegressionExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/GaussianMixtureExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/GBTExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/FeatureHasherExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/FPGrowthExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/FMRegressorExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/FMClassifierExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/EstimatorTransformerParamExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ElementwiseProductExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DeveloperApiExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeRegressionExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeClassificationExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DataFrameExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DCTExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/CountVectorizerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/CorrelationExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ChiSquareTestExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/BucketizerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/BisectingKMeansExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/BinarizerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/AFTSurvivalRegressionExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/SynthBenchmark.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/LiveJournalPageRank.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/ConnectedComponentsExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkTC.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkRemoteFileTest.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkALS.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SkewedGroupByTest.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SimpleSkewedGroupByTest.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/MultiBroadcastTest.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LocalPi.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LocalALS.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/HdfsTest.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/GroupByTest.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ExceptionHandlingTest.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/DFSReadWriteTest.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/BroadcastTest.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/AccumulatorMetricsTest.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewGenerator.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/StreamingExamples.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/RawNetworkGrep.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/QueueStream.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/HdfsWordCount.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKerberizedKafkaWordCount.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/CustomReceiver.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredSessionization.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKerberizedKafkaWordCount.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKafkaWordCount.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/hive/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedScalar.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/RDDRelation.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/SimpleTypedAggregator.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/pythonconverters/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/pythonconverters/AvroConverters.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/users.parquet\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/users.orc\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/users.avro\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/user.avsc\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/people.txt\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/people.json\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/people.csv\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/kv1.txt\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/full_user.avsc\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/employees.json\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/dir1/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/dir1/file3.json\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/dir1/file1.parquet\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/dir1/dir2/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/dir1/dir2/file2.parquet\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/streaming/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/streaming/structured_network_wordcount.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/svmLinear.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/survreg.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/randomForest.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/prefixSpan.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/powerIterationClustering.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/naiveBayes.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/mlp.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/ml.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/logit.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/lm_with_elastic_net.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/lda.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/kstest.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/kmeans.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/isoreg.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/glm.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/gbt.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/gaussianMixture.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/fpm.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/fmRegressor.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/fmClassifier.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/decisionTree.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/bisectingKmeans.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/als.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/dataframe.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/data-manipulation.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/RSparkSQLExample.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/wordcount.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/transitive_closure.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/streaming/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/streaming/stateful_network_wordcount.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/streaming/sql_network_wordcount.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/streaming/recoverable_network_wordcount.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/streaming/queue_stream.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/streaming/network_wordjoinsentiments.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/streaming/network_wordcount.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/streaming/hdfs_wordcount.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/status_api_demo.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/sql/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/sql/streaming/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/sql/streaming/structured_network_wordcount.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/sql/streaming/structured_kafka_wordcount.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/sql/hive.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/sql/basic.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/sql/arrow.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/sql/datasource.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/sort.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/pi.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/parquet_inputformat.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/pagerank.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/word2vec_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/word2vec.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/tf_idf_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/svm_with_sgd_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/svd_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/summary_statistics_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/streaming_linear_regression_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/streaming_k_means_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/stratified_sampling_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/standard_scaler_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/sampled_rdds.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/regression_metrics_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/recommendation_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/ranking_metrics_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/random_rdd_generation.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/random_forest_regression_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/random_forest_classification_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/power_iteration_clustering_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/pca_rowmatrix_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/normalizer_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/naive_bayes_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/multi_label_metrics_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/multi_class_metrics_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/logistic_regression.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/linear_regression_with_sgd_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/latent_dirichlet_allocation_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/kmeans.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/kernel_density_estimation_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/k_means_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/isotonic_regression_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/hypothesis_testing_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/gradient_boosting_regression_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/gradient_boosting_classification_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/gaussian_mixture_model.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/gaussian_mixture_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/fpgrowth_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/elementwise_product_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/decision_tree_regression_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/decision_tree_classification_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/correlations_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/correlations.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/bisecting_k_means_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/binary_classification_metrics_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/word2vec_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/vector_slicer_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/vector_size_hint_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/vector_indexer_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/vector_assembler_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/variance_threshold_selector_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/univariate_feature_selector_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/train_validation_split.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/tokenizer_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/tf_idf_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/summarizer_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/string_indexer_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/stopwords_remover_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/standard_scaler_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/sql_transformer.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/robust_scaler_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/rformula_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/random_forest_regressor_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/random_forest_classifier_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/quantile_discretizer_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/prefixspan_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/power_iteration_clustering_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/polynomial_expansion_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/pipeline_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/pca_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/onehot_encoder_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/one_vs_rest_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/normalizer_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/naive_bayes_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/n_gram_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/multilayer_perceptron_classification.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/multiclass_logistic_regression_with_elastic_net.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/min_max_scaler_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/min_hash_lsh_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/max_abs_scaler_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/logistic_regression_with_elastic_net.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/logistic_regression_summary_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/linearsvc.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/linear_regression_with_elastic_net.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/lda_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/kmeans_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/isotonic_regression_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/interaction_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/index_to_string_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/imputer_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/gradient_boosted_tree_regressor_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/generalized_linear_regression_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/gaussian_mixture_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/fpgrowth_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/fm_regressor_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/fm_classifier_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/feature_hasher_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/estimator_transformer_param_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/elementwise_product_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/decision_tree_regression_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/decision_tree_classification_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/dct_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/dataframe_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/cross_validator.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/count_vectorizer_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/correlation_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/chisq_selector_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/chi_square_test_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/bucketizer_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/bucketed_random_projection_lsh_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/bisecting_k_means_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/binarizer_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/als_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/aft_survival_regression.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/logistic_regression.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/kmeans.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/avro_inputformat.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/als.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scripts/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scripts/getGpusResources.sh\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaStatefulNetworkWordCount.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaSqlNetworkWordCount.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecord.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaQueueStream.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKerberizedKafkaWordCount.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKafkaWordCount.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaCustomReceiver.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredSessionization.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCount.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKerberizedKafkaWordCount.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKafkaWordCount.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/hive/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedScalar.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaSummaryStatisticsExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaStreamingTestExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaStratifiedSamplingExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaSimpleFPGrowth.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVMWithSGDExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaRankingMetricsExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestRegressionExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestClassificationExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaPrefixSpanExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaPowerIterationClusteringExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaNaiveBayesExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaMulticlassClassificationMetricsExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaMultiLabelClassificationMetricsExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaLogisticRegressionWithLBFGSExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaLatentDirichletAllocationExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaLBFGSExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaKernelDensityEstimationExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaKMeansExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaIsotonicRegressionExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingKolmogorovSmirnovTestExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingRegressionExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingClassificationExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaGaussianMixtureExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaElementwiseProductExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeRegressionExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeClassificationExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaCorrelationsExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaChiSqSelectorExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaBisectingKMeansExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaBinaryClassificationMetricsExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaAssociationRulesExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaALS.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaWord2VecExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSlicerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSizeHintExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorIndexerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorAssemblerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaVarianceThresholdSelectorExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaUnivariateFeatureSelectorExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaTfIdfExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaSummarizerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaStringIndexerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaStandardScalerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaSQLTransformerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaRobustScalerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestClassifierExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaRFormulaExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaQuantileDiscretizerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaPrefixSpanExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaPowerIterationClusteringExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaPolynomialExpansionExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaPCAExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaOneVsRestExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaOneHotEncoderExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaNaiveBayesExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaNGramExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaMultilayerPerceptronClassifierExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaMulticlassLogisticRegressionWithElasticNetExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaTrainValidationSplitExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionWithElasticNetExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionSummaryExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearSVCExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearRegressionWithElasticNetExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLabeledDocument.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLDAExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaIsotonicRegressionExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaInteractionExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaIndexToStringExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaImputerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeClassifierExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaGeneralizedLinearRegressionExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaGaussianMixtureExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaFeatureHasherExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaFPGrowthExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaFMRegressorExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaFMClassifierExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaElementwiseProductExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaDocument.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaDCTExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaCountVectorizerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaCorrelationExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSquareTestExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSqSelectorExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketizerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketedRandomProjectionLSHExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaBinarizerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaALSExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaAFTSurvivalRegressionExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaTC.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaStatusTrackerDemo.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaSparkPi.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaPageRank.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaLogQuery.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaHdfsLR.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/jars/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/jars/spark-examples_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/examples/jars/scopt_2.12-3.7.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/tests/\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/tests/worker_memory_check.py\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/tests/python_executable_check.py\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/tests/pyfiles.py\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/tests/py_container_checks.py\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/tests/decommissioning_cleanup.py\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/tests/decommissioning.py\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/tests/autoscale.py\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/dockerfiles/\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/dockerfiles/spark/\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/dockerfiles/spark/bindings/\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/dockerfiles/spark/bindings/python/\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/dockerfiles/spark/bindings/python/Dockerfile\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/dockerfiles/spark/bindings/R/\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/dockerfiles/spark/bindings/R/Dockerfile\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/dockerfiles/spark/Dockerfile\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/dockerfiles/spark/entrypoint.sh\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/dockerfiles/spark/decom.sh\n",
            "spark-3.1.2-bin-hadoop3.2/yarn/\n",
            "spark-3.1.2-bin-hadoop3.2/yarn/spark-3.1.2-yarn-shuffle.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/\n",
            "spark-3.1.2-bin-hadoop3.2/jars/zstd-jni-1.4.8-1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/zookeeper-3.4.14.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/zjsonpatch-0.3.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/xz-1.5.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/xbean-asm7-shaded-4.15.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/woodstox-core-5.0.3.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/velocity-1.5.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/univocity-parsers-2.9.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/transaction-api-1.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/token-provider-1.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/threeten-extra-1.5.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/super-csv-2.2.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/stream-2.9.6.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/stax2-api-3.1.4.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/stax-api-1.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spire_2.12-0.17.0-M1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spire-util_2.12-0.17.0-M1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spire-platform_2.12-0.17.0-M1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spire-macros_2.12-0.17.0-M1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-yarn_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-tags_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-tags_2.12-3.1.2-tests.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-streaming_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-sql_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-sketch_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-repl_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-network-shuffle_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-network-common_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-mllib_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-mllib-local_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-mesos_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-launcher_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-kvstore_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-kubernetes_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-hive_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-hive-thriftserver_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-graphx_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-core_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-catalyst_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/snappy-java-1.1.8.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/snakeyaml-1.24.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/slf4j-log4j12-1.7.30.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/slf4j-api-1.7.30.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/shims-0.9.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/shapeless_2.12-2.3.3.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/scala-xml_2.12-1.2.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/scala-reflect-2.12.10.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/scala-parser-combinators_2.12-1.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/scala-library-2.12.10.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/scala-compiler-2.12.10.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/scala-collection-compat_2.12-2.1.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/re2j-1.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/pyrolite-4.30.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/py4j-0.10.9.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/protobuf-java-2.5.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/parquet-jackson-1.10.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/parquet-hadoop-1.10.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/parquet-format-2.4.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/parquet-encoding-1.10.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/parquet-common-1.10.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/parquet-column-1.10.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/paranamer-2.8.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/osgi-resource-locator-1.0.3.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/oro-2.0.8.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/orc-shims-1.5.12.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/orc-mapreduce-1.5.12.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/orc-core-1.5.12.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/opencsv-2.3.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/okio-1.14.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/okhttp-3.12.12.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/okhttp-2.7.5.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/objenesis-2.6.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/nimbus-jose-jwt-4.41.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/netty-all-4.1.51.Final.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/minlog-1.3.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/metrics-jvm-4.1.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/metrics-json-4.1.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/metrics-jmx-4.1.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/metrics-graphite-4.1.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/metrics-core-4.1.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/mesos-1.4.0-shaded-protobuf.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/macro-compat_2.12-1.1.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/machinist_2.12-0.6.8.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/lz4-java-1.7.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/logging-interceptor-3.12.12.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/log4j-1.2.17.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/libthrift-0.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/libfb303-0.9.3.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/leveldbjni-all-1.8.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-storageclass-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-settings-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-scheduling-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-rbac-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-policy-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-networking-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-metrics-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-extensions-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-events-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-discovery-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-core-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-coordination-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-common-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-certificates-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-batch-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-autoscaling-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-apps-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-apiextensions-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-admissionregistration-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-client-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kryo-shaded-4.0.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kerby-xdr-1.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kerby-util-1.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kerby-pkix-1.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kerby-config-1.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kerby-asn1-1.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kerb-util-1.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kerb-simplekdc-1.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kerb-server-1.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kerb-identity-1.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kerb-crypto-1.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kerb-core-1.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kerb-common-1.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kerb-client-1.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kerb-admin-1.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jul-to-slf4j-1.7.30.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jta-1.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jsr305-3.0.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jsp-api-2.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/json4s-scalap_2.12-3.7.0-M5.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/json4s-jackson_2.12-3.7.0-M5.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/json4s-core_2.12-3.7.0-M5.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/json4s-ast_2.12-3.7.0-M5.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/json-smart-2.3.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/json-1.8.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jpam-1.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jodd-core-3.5.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/joda-time-2.10.5.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jline-2.14.6.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jersey-server-2.30.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jersey-media-jaxb-2.30.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jersey-hk2-2.30.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jersey-container-servlet-core-2.30.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jersey-container-servlet-2.30.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jersey-common-2.30.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jersey-client-2.30.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jdo-api-3.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jcl-over-slf4j-1.7.30.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jcip-annotations-1.0-1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jaxb-runtime-2.3.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jaxb-api-2.2.11.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/javolution-5.5.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/javax.jdo-3.2.0-m3.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/javax.inject-1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/javassist-3.25.0-GA.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/janino-3.0.16.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jakarta.xml.bind-api-2.3.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jakarta.ws.rs-api-2.1.6.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jakarta.validation-api-2.0.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jakarta.servlet-api-4.0.3.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jakarta.inject-2.6.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jakarta.annotation-api-1.3.5.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jakarta.activation-api-1.2.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jackson-module-scala_2.12-2.10.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jackson-module-paranamer-2.10.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jackson-module-jaxb-annotations-2.10.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jackson-mapper-asl-1.9.13.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jackson-jaxrs-json-provider-2.9.5.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jackson-jaxrs-base-2.9.5.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jackson-datatype-jsr310-2.11.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jackson-dataformat-yaml-2.10.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jackson-databind-2.10.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jackson-core-asl-1.9.13.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jackson-core-2.10.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jackson-annotations-2.10.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/ivy-2.4.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/istack-commons-runtime-3.0.8.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/httpcore-4.4.12.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/httpclient-4.5.6.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/htrace-core4-4.1.0-incubating.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hk2-utils-2.6.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hk2-locator-2.6.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hk2-api-2.6.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hive-vector-code-gen-2.3.7.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hive-storage-api-2.7.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hive-shims-scheduler-2.3.7.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hive-shims-common-2.3.7.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hive-shims-2.3.7.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hive-shims-0.23-2.3.7.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hive-service-rpc-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hive-serde-2.3.7.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hive-metastore-2.3.7.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hive-llap-common-2.3.7.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hive-jdbc-2.3.7.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hive-exec-2.3.7-core.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hive-common-2.3.7.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hive-cli-2.3.7.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hive-beeline-2.3.7.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hadoop-yarn-server-web-proxy-3.2.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hadoop-yarn-server-common-3.2.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hadoop-yarn-registry-3.2.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hadoop-yarn-common-3.2.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hadoop-yarn-client-3.2.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hadoop-yarn-api-3.2.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hadoop-mapreduce-client-jobclient-3.2.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hadoop-mapreduce-client-core-3.2.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hadoop-mapreduce-client-common-3.2.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hadoop-hdfs-client-3.2.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hadoop-common-3.2.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hadoop-client-3.2.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hadoop-auth-3.2.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hadoop-annotations-3.2.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/guice-servlet-4.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/guice-4.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/guava-14.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/gson-2.2.4.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/geronimo-jcache_1.0_spec-1.0-alpha-1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/generex-1.0.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/flatbuffers-java-1.9.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/ehcache-3.3.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/dnsjava-2.1.7.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/derby-10.12.1.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/datanucleus-rdbms-4.1.19.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/datanucleus-core-4.1.17.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/datanucleus-api-jdo-4.2.4.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/curator-recipes-2.13.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/curator-framework-2.13.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/curator-client-2.13.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/core-1.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/compress-lzf-1.0.3.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-text-1.6.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-pool-1.5.4.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-net-3.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-math3-3.4.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-logging-1.1.3.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-lang3-3.10.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-lang-2.6.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-io-2.5.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-httpclient-3.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-dbcp-1.4.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-daemon-1.0.13.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-crypto-1.1.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-configuration2-2.1.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-compress-1.20.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-compiler-3.0.16.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-collections-3.2.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-codec-1.10.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-cli-1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-beanutils-1.9.4.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/chill_2.12-0.9.5.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/chill-java-0.9.5.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/cats-kernel_2.12-2.0.0-M4.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/breeze_2.12-1.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/breeze-macros_2.12-1.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/bonecp-0.8.0.RELEASE.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/avro-mapred-1.8.2-hadoop2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/avro-ipc-1.8.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/avro-1.8.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/automaton-1.11-8.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/audience-annotations-0.5.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/arrow-vector-2.0.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/arrow-memory-netty-2.0.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/arrow-memory-core-2.0.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/arrow-format-2.0.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/arpack_combined_all-0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/aopalliance-repackaged-2.6.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/aopalliance-1.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/antlr4-runtime-4.8-1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/antlr-runtime-3.5.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/algebra_2.12-2.0.0-M2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/aircompressor-0.10.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/activation-1.1.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/accessors-smart-1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/ST4-4.0.4.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/RoaringBitmap-0.9.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/JTransforms-3.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/JLargeArrays-1.5.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/HikariCP-2.5.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/RELEASE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbisuSqiQvFe",
        "outputId": "a8e82b95-78a6-41dc-9588-21918bdd31c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls -al"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 223492\n",
            "drwxr-xr-x  1 root root      4096 Sep 10 05:47 .\n",
            "drwxr-xr-x  1 root root      4096 Sep 10 05:45 ..\n",
            "drwxr-xr-x  4 root root      4096 Sep  1 19:26 .config\n",
            "drwxr-xr-x  1 root root      4096 Sep  1 19:26 sample_data\n",
            "drwxr-xr-x 13 1000 1000      4096 May 24 04:45 spark-3.1.2-bin-hadoop3.2\n",
            "-rw-r--r--  1 root root 228834641 May 24 05:01 spark-3.1.2-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmAjcCMsQwQH",
        "outputId": "bb153538-fa53-4c6c-917c-acfcaf2513ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVTUwGPHPbSV"
      },
      "source": [
        "\n",
        "import os\n",
        "#os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n",
        "\n",
        "#!cd spark-3.1.2-bin-hadoop3.2\n",
        "# export SPARK_HOME=`pwd`\n",
        "#os.environ[\"SPARK_HOME\"] = `pwd`\n",
        "# export PYTHONPATH=$(ZIPS=(\"$SPARK_HOME\"/python/lib/*.zip); IFS=:; echo \"${ZIPS[*]}\"):$PYTHONPATH\n",
        "os.environ[\"PYTHONPATH\"] = \"$(ZIPS=(\\\"$SPARK_HOME\\\"/python/lib/*.zip); IFS=:; echo \\\"${ZIPS[*]}\\\"):$PYTHONPATH\""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXIhBVTXRVQy"
      },
      "source": [
        "!pip install -q findspark"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8A3AzbZeQhvY"
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbRIu4xkU2qN"
      },
      "source": [
        "**Not on Colab you should start form HERE:**\n",
        "\n",
        "Reading a text file textFile() in Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMHp9gMPUyPr"
      },
      "source": [
        "from pyspark import SparkContext\n",
        "from operator import add\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Learning_Spark\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# sc = SparkContext.getOrCreate()\n",
        "\n",
        "# sc = spark.sparkContext\n",
        "#lines = sc.textFile(\"spark-2.4.4-bin-hadoop2.7/README.md\")\n",
        "lines = sc.textFile(\"spark-3.1.2-bin-hadoop3.2/README.md\")"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPoJs6cguwyh"
      },
      "source": [
        "Loading all the .md files in one directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3ccsKyvu0qT"
      },
      "source": [
        "input = sc.textFile(\"spark-3.1.2-bin-hadoop3.2/*.md\")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f8uBm53VM1H"
      },
      "source": [
        "**Only in Google Colab:**\n",
        "\n",
        "Load the example1.json JSON file (found on iCollege under Datasets)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeQp5sG3VKqR",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "4295aada-05a4-4de1-a639-904fe5064490"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "# C:\\Users\\jwoo5\\projects\\TrafficJamPrediction\\Waze Traffic Test (3).json"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2de8e776-7e6f-4302-8ae7-da4ec8d24817\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2de8e776-7e6f-4302-8ae7-da4ec8d24817\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Waze Traffic Test (3).json to Waze Traffic Test (3) (1).json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Waze Traffic Test (3).json': b'\\xef\\xbb\\xbf{\"paragraphs\":[{\"text\":\"%md\\\\n## Waze Traffic Prediction \\\\n# Classification of Traffic Jams based on user\\'s data from navigation app\\\\n##### Dalya Dauletbak (dmanato@calstatela.edu)\\\\nRevised 11/1/2019\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:34:44+0000\",\"config\":{\"colWidth\":12,\"fontSize\":9,\"enabled\":true,\"results\":{},\"editorSetting\":{\"language\":\"scala\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"editorMode\":\"ace/mode/scala\",\"editorHide\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"HTML\",\"data\":\"<div class=\\\\\"markdown-body\\\\\">\\\\n<h2>Waze Traffic Prediction</h2>\\\\n<h1>Classification of Traffic Jams based on user&rsquo;s data from navigation app</h1>\\\\n<h5>Dalya Dauletbak (<a href=\\\\\"mailto:&#100;&#109;&#97;&#x6e;&#x61;&#x74;&#x6f;&#64;&#99;a&#x6c;&#115;t&#x61;&#x74;e&#108;&#97;&#46;e&#100;&#x75;\\\\\">&#100;&#109;&#97;&#x6e;&#x61;&#x74;&#x6f;&#64;&#99;a&#x6c;&#115;t&#x61;&#x74;e&#108;&#97;&#46;e&#100;&#x75;</a>)</h5>\\\\n<p>Revised 11/1/2019</p>\\\\n</div>\"}]},\"apps\":[],\"jobName\":\"paragraph_1570440703870_-600274472\",\"id\":\"20191007-093143_1885049358\",\"dateCreated\":\"2019-10-07T09:31:43+0000\",\"dateStarted\":\"2019-10-07T09:34:25+0000\",\"dateFinished\":\"2019-10-07T09:34:25+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"focus\":true,\"$$hashKey\":\"object:172\"},{\"text\":\"%md \\\\n### First step is to parse JSON files into the dataframe. Waze set for December 31, 2017 - February 9, 2018\\\\n\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:34:55+0000\",\"config\":{\"tableHide\":false,\"editorSetting\":{\"language\":\"markdown\",\"editOnDblClick\":true,\"completionKey\":\"TAB\",\"completionSupport\":false},\"colWidth\":12,\"editorMode\":\"ace/mode/markdown\",\"fontSize\":9,\"editorHide\":true,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"HTML\",\"data\":\"<div class=\\\\\"markdown-body\\\\\">\\\\n<h3>First step is to parse JSON files into the dataframe. Waze set for December 31, 2017 - February 9, 2018</h3>\\\\n</div>\"}]},\"apps\":[],\"jobName\":\"paragraph_1570433236998_1758939291\",\"id\":\"20190624-101935_555206611\",\"dateCreated\":\"2019-10-07T07:27:16+0000\",\"dateStarted\":\"2019-10-07T09:34:55+0000\",\"dateFinished\":\"2019-10-07T09:34:55+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:173\"},{\"text\":\"%python\\\\nimport os, json\\\\nimport pandas as pd\\\\nfrom pandas.io.json import json_normalize #package for flattening json in pandas df\\\\nimport s3fs\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T19:33:48+0000\",\"config\":{\"colWidth\":12,\"fontSize\":9,\"enabled\":true,\"results\":{},\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"editorMode\":\"ace/mode/python\"},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"ERROR\",\"msg\":[{\"type\":\"TEXT\",\"data\":\"Traceback (most recent call last):\\\\n  File \\\\\"/tmp/zeppelin_python-3458386589638175072.py\\\\\", line 319, in <module>\\\\n    raise Exception(traceback.format_exc())\\\\nException: Traceback (most recent call last):\\\\n  File \\\\\"/tmp/zeppelin_python-3458386589638175072.py\\\\\", line 312, in <module>\\\\n    exec(code, _zcUserQueryNameSpace)\\\\n  File \\\\\"<stdin>\\\\\", line 4, in <module>\\\\n  File \\\\\"/usr/local/lib/python2.7/site-packages/s3fs/__init__.py\\\\\", line 1, in <module>\\\\n    from .core import S3FileSystem, S3File\\\\n  File \\\\\"/usr/local/lib/python2.7/site-packages/s3fs/core.py\\\\\", line 2, in <module>\\\\n    from six import raise_from\\\\nImportError: cannot import name raise_from\\\\n\\\\n\"}]},\"apps\":[],\"jobName\":\"paragraph_1570440899222_203581449\",\"id\":\"20191007-093459_1923310329\",\"dateCreated\":\"2019-10-07T09:34:59+0000\",\"dateStarted\":\"2019-10-07T19:33:48+0000\",\"dateFinished\":\"2019-10-07T19:33:48+0000\",\"status\":\"ERROR\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:174\"},{\"text\":\"%python\\\\n#path_to_json = \\'/Users/dalyapraz/Desktop/City of LA/Waze/waze json dec31-jan9/\\'\\\\n\\\\n#S3 path to the folder with all JSON files with traffic jams\\\\npath_to_json = \\'s3://waze-full/waze json dec31-feb9/\\'\\\\njson_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith(\\'.json\\')]\\\\n#json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.startswith(\\'wazedata_2018_01_08\\')]\\\\n#json_files = [pos_json for pos_json in os.listdir(path_to_json)]\\\\n#print(len(json_files))  # prints the name of files\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-08T01:16:31+0000\",\"config\":{\"colWidth\":12,\"fontSize\":9,\"enabled\":true,\"results\":{},\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"editorMode\":\"ace/mode/python\"},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"ERROR\",\"msg\":[{\"type\":\"TEXT\",\"data\":\"Traceback (most recent call last):\\\\n  File \\\\\"/tmp/zeppelin_python-3458386589638175072.py\\\\\", line 319, in <module>\\\\n    raise Exception(traceback.format_exc())\\\\nException: Traceback (most recent call last):\\\\n  File \\\\\"/tmp/zeppelin_python-3458386589638175072.py\\\\\", line 312, in <module>\\\\n    exec(code, _zcUserQueryNameSpace)\\\\n  File \\\\\"<stdin>\\\\\", line 2, in <module>\\\\nOSError: [Errno 2] No such file or directory: \\'https://waze-full.s3.amazonaws.com/waze+json+dec31-feb9/\\'\\\\n\\\\n\"}]},\"apps\":[],\"jobName\":\"paragraph_1570442264437_1087534646\",\"id\":\"20191007-095744_1007612181\",\"dateCreated\":\"2019-10-07T09:57:44+0000\",\"dateStarted\":\"2019-10-07T10:11:58+0000\",\"dateFinished\":\"2019-10-07T10:11:58+0000\",\"status\":\"ERROR\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:175\"},{\"text\":\"%python\\\\n#load json object\\\\nwith open(os.path.join(path_to_json, json_files[0])) as first_file:\\\\n    data = json.load(first_file)\\\\nprint(json_files[0]) \",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T17:41:16+0000\",\"config\":{\"colWidth\":12,\"fontSize\":9,\"enabled\":true,\"results\":{},\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"editorMode\":\"ace/mode/python\"},\"settings\":{\"params\":{},\"forms\":{}},\"apps\":[],\"jobName\":\"paragraph_1570470061051_-1311431450\",\"id\":\"20191007-174101_2052718372\",\"dateCreated\":\"2019-10-07T17:41:01+0000\",\"status\":\"READY\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:176\"},{\"text\":\"%python\\\\n#lets put the data into a pandas df\\\\n\\\\ndf_jams = json_normalize(data = data[\\'jams\\'], record_path=[\\'line\\'], \\\\n                         meta = [\\'id\\',\\'pubMillis\\', \\'level\\', \\'speed\\', \\'length\\', \\'delay\\'])\\\\n\\\\nprint(df_jams.info())\\\\nprint(df_jams.head())\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T17:41:29+0000\",\"config\":{\"colWidth\":12,\"fontSize\":9,\"enabled\":true,\"results\":{},\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"editorMode\":\"ace/mode/python\"},\"settings\":{\"params\":{},\"forms\":{}},\"apps\":[],\"jobName\":\"paragraph_1570470076621_1288138972\",\"id\":\"20191007-174116_588421420\",\"dateCreated\":\"2019-10-07T17:41:16+0000\",\"status\":\"READY\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:177\"},{\"text\":\"%python\\\\n# go through all files starting from 2nd json_files[1]\\\\nfor js in json_files[1:]:\\\\n    with open(os.path.join(path_to_json, js)) as json_file:\\\\n        json_data = json.load(json_file)\\\\n        try:\\\\n            df_file= json_normalize(data = json_data[\\'jams\\'], record_path=[\\'line\\'], \\\\n                             meta = [\\'id\\',\\'pubMillis\\', \\'level\\', \\'speed\\', \\'length\\', \\'delay\\'])\\\\n            df_jams = df_jams.append(df_file,ignore_index=True)\\\\n        except:\\\\n            print(\\'no jams for file\\', js)\\\\n            continue\\\\nprint(json_files.index(js))\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T17:41:42+0000\",\"config\":{\"colWidth\":12,\"fontSize\":9,\"enabled\":true,\"results\":{},\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"editorMode\":\"ace/mode/python\"},\"settings\":{\"params\":{},\"forms\":{}},\"apps\":[],\"jobName\":\"paragraph_1570470089264_-1717311465\",\"id\":\"20191007-174129_624294408\",\"dateCreated\":\"2019-10-07T17:41:29+0000\",\"status\":\"READY\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:178\"},{\"text\":\"%python\\\\n# now that we have the pertinent json data in our DataFrame let\\'s look at it\\\\nprint(df_jams.info())\\\\nprint(df_jams.tail())\\\\n#print(df[\\'startTime\\'].head())\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T17:41:56+0000\",\"config\":{\"colWidth\":12,\"fontSize\":9,\"enabled\":true,\"results\":{},\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"editorMode\":\"ace/mode/python\"},\"settings\":{\"params\":{},\"forms\":{}},\"apps\":[],\"jobName\":\"paragraph_1570470102473_581362528\",\"id\":\"20191007-174142_1416889264\",\"dateCreated\":\"2019-10-07T17:41:42+0000\",\"status\":\"READY\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:179\"},{\"text\":\"%python\\\\nimport datetime\\\\ndf_jams.rename(columns={\\'x\\':\\'location_x\\', \\'y\\':\\'location_y\\'}, inplace=True)\\\\ndf_jams[\\'pub_date\\'] = pd.to_datetime(df_jams[\\'pubMillis\\'],  unit=\\'ms\\').dt.tz_localize(tz= \\'UTC\\')\\\\ndf_jams[\\'date_pst\\'] = df_jams[\\'pub_date\\'].dt.tz_convert(\\'US/Pacific\\')\\\\ndf_jams[\\'month\\'] = df_jams[\\'date_pst\\'].dt.month\\\\ndf_jams[\\'day\\'] = df_jams[\\'date_pst\\'].dt.day\\\\ndf_jams[\\'hour\\'] = df_jams[\\'date_pst\\'].dt.hour\\\\ndf_jams[\\'min\\'] = df_jams[\\'date_pst\\'].dt.minute\\\\ndf_jams[\\'sec\\'] = df_jams[\\'date_pst\\'].dt.second\\\\ndf_jams[\\'weekday\\'] = df_jams[\\'date_pst\\'].dt.weekday_name\\\\nprint(df_jams.head())\\\\nprint(df_jams.info())\\\\n#df.to_csv(\\'jams_predict_hdfs.csv\\', encoding=\\'utf-8\\', index=False)\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T17:42:20+0000\",\"config\":{\"colWidth\":12,\"fontSize\":9,\"enabled\":true,\"results\":{},\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"editorMode\":\"ace/mode/python\"},\"settings\":{\"params\":{},\"forms\":{}},\"apps\":[],\"jobName\":\"paragraph_1570470116710_1976762471\",\"id\":\"20191007-174156_1379068118\",\"dateCreated\":\"2019-10-07T17:41:56+0000\",\"status\":\"READY\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:180\"},{\"text\":\"%md\\\\n## Creating a Classification Model\\\\n\\\\nIn this proejct we will implement a binary classification model using *RandomForest* that uses features of location, date and time to predict whether or not there will be traffic jam.\\\\n\\\\nFollowing are the steps below to build, train and test the model from the source data:\\\\n\\\\n1. Prepare the dataset on cluster using Hive\\\\n2. Transform data using SQL Transformer\\\\n3. Prepare the data with the features (input columns, output column as label)\\\\n4. Split the data using data.randomSplit(): Training and Testing\\\\n5. Normalize data and create a vector using VectorAssembler\\\\n6. Set features and label from the vector\\\\n7. Build a RandomForest Model with the label and features\\\\n8. Train the model uisng TrainValidationSplit and CrossValidation\\\\n9. Predict and test the testing Data Frame using the model trained at the step 8\\\\n10. Compare the predicted result and trueLabel\\\\n11. Build a confusion matrix and calculate Recall and Precision\\\\n12. Calculate AUC - area under the curve\\\\n\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:15:27+0000\",\"config\":{\"tableHide\":false,\"editorSetting\":{\"language\":\"markdown\",\"editOnDblClick\":true,\"completionKey\":\"TAB\",\"completionSupport\":false},\"colWidth\":12,\"editorMode\":\"ace/mode/markdown\",\"fontSize\":9,\"editorHide\":true,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"HTML\",\"data\":\"<div class=\\\\\"markdown-body\\\\\">\\\\n<h2>Creating a Classification Model</h2>\\\\n<p>In this proejct we will implement a binary classification model using <em>RandomForest</em> that uses features of location, date and time to predict whether or not there will be traffic jam.</p>\\\\n<p>Following are the steps below to build, train and test the model from the source data:</p>\\\\n<ol>\\\\n  <li>Prepare the dataset on cluster using Hive</li>\\\\n  <li>Transform data using SQL Transformer</li>\\\\n  <li>Prepare the data with the features (input columns, output column as label)</li>\\\\n  <li>Split the data using data.randomSplit(): Training and Testing</li>\\\\n  <li>Normalize data and create a vector using VectorAssembler</li>\\\\n  <li>Set features and label from the vector</li>\\\\n  <li>Build a RandomForest Model with the label and features</li>\\\\n  <li>Train the model uisng TrainValidationSplit and CrossValidation</li>\\\\n  <li>Predict and test the testing Data Frame using the model trained at the step 8</li>\\\\n  <li>Compare the predicted result and trueLabel</li>\\\\n  <li>Build a confusion matrix and calculate Recall and Precision</li>\\\\n  <li>Calculate AUC - area under the curve</li>\\\\n</ol>\\\\n</div>\"}]},\"apps\":[],\"jobName\":\"paragraph_1570433237002_-1066286651\",\"id\":\"20190624-102754_1732483831\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:15:27+0000\",\"dateFinished\":\"2019-10-07T09:15:27+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:181\"},{\"text\":\"%md\\\\n## Import Spark SQL and Spark ML Libraries\\\\n\\\\nFirst, import the libraries you will need:\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:15:27+0000\",\"config\":{\"tableHide\":false,\"editorSetting\":{\"language\":\"markdown\",\"editOnDblClick\":true,\"completionKey\":\"TAB\",\"completionSupport\":false},\"colWidth\":12,\"editorMode\":\"ace/mode/markdown\",\"fontSize\":9,\"editorHide\":true,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"HTML\",\"data\":\"<div class=\\\\\"markdown-body\\\\\">\\\\n<h2>Import Spark SQL and Spark ML Libraries</h2>\\\\n<p>First, import the libraries you will need:</p>\\\\n</div>\"}]},\"apps\":[],\"jobName\":\"paragraph_1570433237002_2010878462\",\"id\":\"20190624-102856_803244533\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:15:27+0000\",\"dateFinished\":\"2019-10-07T09:15:27+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:182\"},{\"text\":\"%spark.pyspark\\\\n\\\\n# Import Spark SQL and Spark ML libraries\\\\nfrom pyspark.sql.types import *\\\\nfrom pyspark.sql.functions import *\\\\n\\\\nfrom pyspark.ml import Pipeline\\\\nfrom pyspark.ml.classification import DecisionTreeClassifier\\\\nfrom pyspark.ml.classification import RandomForestClassifier\\\\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer, MinMaxScaler, SQLTransformer, Normalizer\\\\nfrom pyspark.ml.feature import VectorAssembler\\\\nfrom pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit, CrossValidator\\\\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\\\\n\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:15:27+0000\",\"config\":{\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"colWidth\":12,\"editorMode\":\"ace/mode/python\",\"fontSize\":9,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[]},\"apps\":[],\"jobName\":\"paragraph_1570433237002_1740933264\",\"id\":\"20190624-102910_1206383417\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:15:27+0000\",\"dateFinished\":\"2019-10-07T09:15:27+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:183\"},{\"text\":\"%md\\\\n\\\\n## Prepare the dataset\\\\nAfter cleaning data in Hive download the dataset on cluster and upload to DataFrame.\\\\\\\\\\\\nAlso upload *holidays* dataset and Join two datasets.\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:15:27+0000\",\"config\":{\"tableHide\":false,\"editorSetting\":{\"language\":\"markdown\",\"editOnDblClick\":true,\"completionKey\":\"TAB\",\"completionSupport\":false},\"colWidth\":12,\"editorMode\":\"ace/mode/markdown\",\"fontSize\":9,\"editorHide\":true,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"HTML\",\"data\":\"<div class=\\\\\"markdown-body\\\\\">\\\\n<h2>Prepare the dataset</h2>\\\\n<p>After cleaning data in Hive download the dataset on cluster and upload to DataFrame.\\\\\\\\<br/>Also upload <em>holidays</em> dataset and Join two datasets.</p>\\\\n</div>\"}]},\"apps\":[],\"jobName\":\"paragraph_1570433237003_1894559275\",\"id\":\"20190624-103001_981598499\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:15:27+0000\",\"dateFinished\":\"2019-10-07T09:15:27+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:184\"},{\"text\":\"%spark.pyspark\\\\n\\\\n# file = spark.read.csv(\\'s3://jwoo-wazetest/jamsPredictCleanFullHead.csv\\',inferSchema=True, header=True)\\\\nfile = spark.read.csv(\\'s3://wazetest/jamsPredictCleanFullHead.csv\\',inferSchema=True, header=True)\\\\nfile.show(5)\\\\nholidays = spark.read.csv(\\'s3://wazetest/holidays_2018.csv\\', inferSchema=True, header=True)\\\\njoinCsv = file.join(holidays, (file.month == holidays.month) & (file.day == holidays.day), how = \\\\\"left\\\\\").select(file.location_x, file.location_y,file.pub_millis,  file.level, file.speed, file.pub_date, file.date_pst, file.month, file.day, file.hour, file.min, file.sec, file.weekday, holidays.Comments)\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:15:27+0000\",\"config\":{\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"colWidth\":12,\"editorMode\":\"ace/mode/python\",\"fontSize\":9,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"TEXT\",\"data\":\"+-------------------+------------------+---------+-------------+-----+-----+------+-----+--------------------+--------------------+-----+---+----+---+---+---------+\\\\n|         location_x|        location_y|       id|   pub_millis|level|speed|length|delay|            pub_date|            date_pst|month|day|hour|min|sec|  weekday|\\\\n+-------------------+------------------+---------+-------------+-----+-----+------+-----+--------------------+--------------------+-----+---+----+---+---+---------+\\\\n|-118.07918500000001|34.148016999999996|689767350|1515091705037|    2|   15|  6048|  190|2018-01-04 18:48:...|2018-01-04 10:48:...|    1|  4|  10| 48| 25| Thursday|\\\\n|        -118.082976|         33.774503|789709479|1515431250767|    3|   13| 32050| 1421|2018-01-08 17:07:...|2018-01-08 09:07:...|    1|  8|   9|  7| 30|   Monday|\\\\n|-118.08305700000001|         34.147963|636265740|1514927453466|    2|   12|  5810|  271|2018-01-02 21:10:...|2018-01-02 13:10:...|    1|  2|  13| 10| 53|  Tuesday|\\\\n|-118.08373600000002|         34.148038|671467851|1515031086196|    2|   11|  6350|  326|2018-01-04 01:58:...|2018-01-03 17:58:...|    1|  3|  17| 58|  6|Wednesday|\\\\n|        -118.083992|         33.774538|777091475|1515430666323|    3|   13| 32050| 1460|2018-01-08 16:57:...|2018-01-08 08:57:...|    1|  8|   8| 57| 46|   Monday|\\\\n+-------------------+------------------+---------+-------------+-----+-----+------+-----+--------------------+--------------------+-----+---+----+---+---+---------+\\\\nonly showing top 5 rows\\\\n\\\\n\"}]},\"runtimeInfos\":{\"jobUrl\":{\"propertyName\":\"jobUrl\",\"label\":\"SPARK JOB\",\"tooltip\":\"View in Spark web UI\",\"group\":\"spark\",\"values\":[\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=462\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=463\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=464\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=465\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=466\"],\"interpreterSettingId\":\"spark\"}},\"apps\":[],\"jobName\":\"paragraph_1570433237003_-190699011\",\"id\":\"20190624-103108_672361149\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:15:27+0000\",\"dateFinished\":\"2019-10-07T09:15:53+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:185\"},{\"text\":\"%md\\\\n\\\\n## Transform Data  using SQLTransformer\\\\n1. Change *weekday* to numerical values \\\\n2. Create *is_holiday* column, where 1 - for holiday and 0 - for non-holiday\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:15:53+0000\",\"config\":{\"tableHide\":false,\"editorSetting\":{\"language\":\"markdown\",\"editOnDblClick\":true,\"completionKey\":\"TAB\",\"completionSupport\":false},\"colWidth\":12,\"editorMode\":\"ace/mode/markdown\",\"fontSize\":9,\"editorHide\":true,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"HTML\",\"data\":\"<div class=\\\\\"markdown-body\\\\\">\\\\n<h2>Transform Data using SQLTransformer</h2>\\\\n<ol>\\\\n  <li>Change <em>weekday</em> to numerical values</li>\\\\n  <li>Create <em>is_holiday</em> column, where 1 - for holiday and 0 - for non-holiday</li>\\\\n</ol>\\\\n</div>\"}]},\"apps\":[],\"jobName\":\"paragraph_1570433237003_1612105529\",\"id\":\"20190624-103133_1586313964\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:15:53+0000\",\"dateFinished\":\"2019-10-07T09:15:53+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:186\"},{\"text\":\"%spark.pyspark\\\\n\\\\n# Transform data using SQLTransformer\\\\n# comment this part for running code in Oracle\\\\n\\\\nsql = SQLTransformer(\\\\n    statement=\\\\\"SELECT location_x, location_y, pub_millis, level, speed, pub_date, date_pst, month, day, hour, min, sec,\\\\\\\\\\\\n  CASE \\\\\\\\\\\\n      WHEN weekday = \\'Monday\\' THEN 0 \\\\\\\\\\\\n      WHEN weekday = \\'Tuesday\\' THEN 1 \\\\\\\\\\\\n      WHEN weekday = \\'Wednesday\\' THEN 2 \\\\\\\\\\\\n      WHEN weekday = \\'Thursday\\' THEN 3 \\\\\\\\\\\\n      WHEN weekday = \\'Friday\\' THEN 4 \\\\\\\\\\\\n      WHEN weekday = \\'Saturday\\' THEN 5 \\\\\\\\\\\\n      WHEN weekday = \\'Sunday\\' THEN 6 \\\\\\\\\\\\n  END AS weekday, \\\\\\\\\\\\n  CASE \\\\\\\\\\\\n      WHEN Comments IS NOT NULL THEN 1 \\\\\\\\\\\\n      ELSE 0 \\\\\\\\\\\\n  END AS is_holiday \\\\\\\\\\\\n FROM __THIS__\\\\\") \\\\n \\\\ncsv = sql.transform(joinCsv)\\\\n\\\\ncsv.show(5)\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:15:53+0000\",\"config\":{\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"colWidth\":12,\"editorMode\":\"ace/mode/python\",\"fontSize\":9,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"TEXT\",\"data\":\"+-------------------+------------------+-------------+-----+-----+--------------------+--------------------+-----+---+----+---+---+-------+----------+\\\\n|         location_x|        location_y|   pub_millis|level|speed|            pub_date|            date_pst|month|day|hour|min|sec|weekday|is_holiday|\\\\n+-------------------+------------------+-------------+-----+-----+--------------------+--------------------+-----+---+----+---+---+-------+----------+\\\\n|-118.07918500000001|34.148016999999996|1515091705037|    2|   15|2018-01-04 18:48:...|2018-01-04 10:48:...|    1|  4|  10| 48| 25|      3|         0|\\\\n|        -118.082976|         33.774503|1515431250767|    3|   13|2018-01-08 17:07:...|2018-01-08 09:07:...|    1|  8|   9|  7| 30|      0|         0|\\\\n|-118.08305700000001|         34.147963|1514927453466|    2|   12|2018-01-02 21:10:...|2018-01-02 13:10:...|    1|  2|  13| 10| 53|      1|         0|\\\\n|-118.08373600000002|         34.148038|1515031086196|    2|   11|2018-01-04 01:58:...|2018-01-03 17:58:...|    1|  3|  17| 58|  6|      2|         0|\\\\n|        -118.083992|         33.774538|1515430666323|    3|   13|2018-01-08 16:57:...|2018-01-08 08:57:...|    1|  8|   8| 57| 46|      0|         0|\\\\n+-------------------+------------------+-------------+-----+-----+--------------------+--------------------+-----+---+----+---+---+-------+----------+\\\\nonly showing top 5 rows\\\\n\\\\n\"}]},\"runtimeInfos\":{\"jobUrl\":{\"propertyName\":\"jobUrl\",\"label\":\"SPARK JOB\",\"tooltip\":\"View in Spark web UI\",\"group\":\"spark\",\"values\":[\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=467\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=468\"],\"interpreterSettingId\":\"spark\"}},\"apps\":[],\"jobName\":\"paragraph_1570433237003_1211379440\",\"id\":\"20190624-103224_465745511\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:15:53+0000\",\"dateFinished\":\"2019-10-07T09:15:53+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:187\"},{\"text\":\"%md \\\\n## Create label column and correct DataTypes\\\\n1. Change the datatypes approprietly\\\\n2. Create label column based on ***level***. \\\\n\\\\n***Level*** has numirecal values from 1 to 5, indicating the level of the jam captured by the app based on user\\'s speed, delay from the original route. However levels 1and 2 are too low and can be neglected as traffic. Thus we will assign label column with *1* - for values greater than 2, and *0* - for the values equal or less than 2.\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:15:53+0000\",\"config\":{\"tableHide\":false,\"editorSetting\":{\"language\":\"markdown\",\"editOnDblClick\":true,\"completionKey\":\"TAB\",\"completionSupport\":false},\"colWidth\":12,\"editorMode\":\"ace/mode/markdown\",\"fontSize\":9,\"editorHide\":true,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"HTML\",\"data\":\"<div class=\\\\\"markdown-body\\\\\">\\\\n<h2>Create label column and correct DataTypes</h2>\\\\n<ol>\\\\n  <li>Change the datatypes approprietly</li>\\\\n  <li>Create label column based on <strong><em>level</em></strong>.</li>\\\\n</ol>\\\\n<p><strong><em>Level</em></strong> has numirecal values from 1 to 5, indicating the level of the jam captured by the app based on user&rsquo;s speed, delay from the original route. However levels 1and 2 are too low and can be neglected as traffic. Thus we will assign label column with <em>1</em> - for values greater than 2, and <em>0</em> - for the values equal or less than 2.</p>\\\\n</div>\"}]},\"apps\":[],\"jobName\":\"paragraph_1570433237004_-1770007293\",\"id\":\"20190624-103312_811609646\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:15:53+0000\",\"dateFinished\":\"2019-10-07T09:15:53+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:188\"},{\"text\":\"%spark.pyspark\\\\n\\\\n# Select features and label and assign label = 0 & label = 1\\\\ndata = csv.select((col(\\\\\"location_x\\\\\").cast(\\\\\"Double\\\\\")),(col(\\\\\"location_y\\\\\").cast(\\\\\"Double\\\\\")), (col(\\\\\"pub_millis\\\\\").cast(\\\\\"Float\\\\\")), (col(\\\\\"month\\\\\").cast(\\\\\"Integer\\\\\")), (col(\\\\\"day\\\\\").cast(\\\\\"Integer\\\\\")), (col(\\\\\"hour\\\\\").cast(\\\\\"Integer\\\\\")), (col(\\\\\"min\\\\\").cast(\\\\\"Integer\\\\\")), (col(\\\\\"sec\\\\\").cast(\\\\\"Integer\\\\\")),(col(\\\\\"weekday\\\\\").cast(\\\\\"Integer\\\\\")),(col(\\\\\"is_holiday\\\\\").cast(\\\\\"Integer\\\\\")), (col(\\\\\"level\\\\\").cast(\\\\\"Integer\\\\\")),((col(\\\\\"level\\\\\") > 2).cast(\\\\\"Integer\\\\\").alias(\\\\\"label\\\\\")))\\\\ndata.show()\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:15:53+0000\",\"config\":{\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"colWidth\":12,\"editorMode\":\"ace/mode/python\",\"fontSize\":9,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"TEXT\",\"data\":\"+-------------------+------------------+-------------+-----+---+----+---+---+-------+----------+-----+-----+\\\\n|         location_x|        location_y|   pub_millis|month|day|hour|min|sec|weekday|is_holiday|level|label|\\\\n+-------------------+------------------+-------------+-----+---+----+---+---+-------+----------+-----+-----+\\\\n|-118.07918500000001|34.148016999999996|1.51509166E12|    1|  4|  10| 48| 25|      3|         0|    2|    0|\\\\n|        -118.082976|         33.774503|1.51543126E12|    1|  8|   9|  7| 30|      0|         0|    3|    1|\\\\n|-118.08305700000001|         34.147963|1.51492742E12|    1|  2|  13| 10| 53|      1|         0|    2|    0|\\\\n|-118.08373600000002|         34.148038| 1.5150311E12|    1|  3|  17| 58|  6|      2|         0|    2|    0|\\\\n|        -118.083992|         33.774538|1.51543061E12|    1|  8|   8| 57| 46|      0|         0|    3|    1|\\\\n|        -118.084849|         34.148274|1.51492742E12|    1|  2|  13| 10| 53|      1|         0|    2|    0|\\\\n|        -118.084849|         34.148274|1.51492742E12|    1|  2|  13| 10| 53|      1|         0|    2|    0|\\\\n|        -118.084849|         34.148274|1.51492742E12|    1|  2|  13| 10| 53|      1|         0|    3|    1|\\\\n|        -118.084849|         34.148274|1.51492742E12|    1|  2|  13| 10| 53|      1|         0|    3|    1|\\\\n|        -118.084849|         34.148274|1.51502363E12|    1|  3|  15| 52| 55|      2|         0|    2|    0|\\\\n|        -118.084849|         34.148274|1.51502363E12|    1|  3|  15| 52| 55|      2|         0|    3|    1|\\\\n|        -118.084849|         34.148274|1.51509166E12|    1|  4|  10| 48| 25|      3|         0|    2|    0|\\\\n|        -118.084849|         34.148274|1.51519114E12|    1|  5|  14| 26| 12|      4|         0|    3|    1|\\\\n|-118.08511100000001|         33.774715|1.51543061E12|    1|  8|   8| 57| 46|      0|         0|    3|    1|\\\\n|-118.08523999999998|34.148396999999996|1.51507816E12|    1|  4|   7|  2| 23|      3|         0|    2|    0|\\\\n|-118.08523999999998|34.148396999999996|1.51520595E12|    1|  5|  18| 31| 31|      4|         0|    2|    0|\\\\n|-118.08548700000001|         33.774813|1.51543126E12|    1|  8|   9|  7| 30|      0|         0|    3|    1|\\\\n|        -118.085629|34.148534000000005|1.51492742E12|    1|  2|  13| 10| 53|      1|         0|    2|    0|\\\\n|        -118.085629|34.148534000000005|1.51492742E12|    1|  2|  13| 10| 53|      1|         0|    2|    0|\\\\n|        -118.085629|34.148534000000005|1.51509166E12|    1|  4|  10| 48| 25|      3|         0|    2|    0|\\\\n+-------------------+------------------+-------------+-----+---+----+---+---+-------+----------+-----+-----+\\\\nonly showing top 20 rows\\\\n\\\\n\"}]},\"runtimeInfos\":{\"jobUrl\":{\"propertyName\":\"jobUrl\",\"label\":\"SPARK JOB\",\"tooltip\":\"View in Spark web UI\",\"group\":\"spark\",\"values\":[\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=469\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=470\"],\"interpreterSettingId\":\"spark\"}},\"apps\":[],\"jobName\":\"paragraph_1570433237004_1117233991\",\"id\":\"20190624-103350_677144486\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:15:53+0000\",\"dateFinished\":\"2019-10-07T09:15:54+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:189\"},{\"text\":\"%md\\\\n\\\\n## Transform Cyclical Features using SQLTransformer\\\\nCreating such columns as: \\\\n\\\\n***is_rush*** - *1* if time is between 7am and 9am or 3pm and 6pm, otherwise *0*; \\\\n\\\\n***is_weekend*** - *1* if Saturday or Sunday, otherwise *0*. \\\\n\\\\nTransforming cyclical fields, such as hours, minutes, seconds, weekday number and etc., to the appropriate representation. This can be done by converting features from Polar coordinate system to Cartesian, applying trigonometric functions:\\\\n\\\\n**x=r  sin\\xe2\\x81\\xa1\\xcf\\x86  and y=  cos\\xe2\\x81\\xa1\\xcf\\x86,\\\\nwhere \\xcf\\x86= k 2\\xcf\\x80/n,\\\\nk-value of the field, \\\\nn-number of possible values**\\\\n\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:15:54+0000\",\"config\":{\"tableHide\":false,\"editorSetting\":{\"language\":\"markdown\",\"editOnDblClick\":true,\"completionKey\":\"TAB\",\"completionSupport\":false},\"colWidth\":12,\"editorMode\":\"ace/mode/markdown\",\"fontSize\":9,\"editorHide\":true,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"HTML\",\"data\":\"<div class=\\\\\"markdown-body\\\\\">\\\\n<h2>Transform Cyclical Features using SQLTransformer</h2>\\\\n<p>Creating such columns as: </p>\\\\n<p><strong><em>is_rush</em></strong> - <em>1</em> if time is between 7am and 9am or 3pm and 6pm, otherwise <em>0</em>; </p>\\\\n<p><strong><em>is_weekend</em></strong> - <em>1</em> if Saturday or Sunday, otherwise <em>0</em>. </p>\\\\n<p>Transforming cyclical fields, such as hours, minutes, seconds, weekday number and etc., to the appropriate representation. This can be done by converting features from Polar coordinate system to Cartesian, applying trigonometric functions:</p>\\\\n<p><strong>x=r sin\\xe2\\x81\\xa1\\xcf\\x86 and y= cos\\xe2\\x81\\xa1\\xcf\\x86,<br/>where \\xcf\\x86= k 2\\xcf\\x80/n,<br/>k-value of the field,<br/>n-number of possible values</strong></p>\\\\n</div>\"}]},\"apps\":[],\"jobName\":\"paragraph_1570433237004_2075305633\",\"id\":\"20190624-103405_827852374\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:15:54+0000\",\"dateFinished\":\"2019-10-07T09:15:54+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:190\"},{\"text\":\"%spark.pyspark\\\\n\\\\n#apply transformation of cyclical features using cos/sin\\\\nsqlTrans = SQLTransformer(\\\\n    statement=\\\\\"SELECT location_x, location_y, SIN((weekday)*(2*PI()/7)) as sin_weekday, COS((weekday)*(2*PI()/7)) as cos_weekday, SIN((month-1)*(2*PI()/12)) as sin_month, COS((month-1)*(2*PI()/12)) as cos_month, SIN((day-1)*(2*PI()/31)) as sin_day, COS((day-1)*(2*PI()/31)) as cos_day, SIN(hour*(2*PI()/24)) as sin_hour, COS(hour*(2*PI()/24)) as cos_hour, SIN(min*(2*PI()/60)) as sin_min, COS(min*(2*PI()/60)) as cos_min , SIN(sec*(2*PI()/60)) as sin_sec, COS(sec*(2*PI()/60)) as cos_sec,\\\\\\\\\\\\n   CASE \\\\\\\\\\\\n    WHEN (hour+min/60) >= 7 and (hour+min/60) <= 9 THEN 1 \\\\\\\\\\\\n    WHEN (hour+min/60) >= 15 and (hour+min/60) <= 18 THEN 1 \\\\\\\\\\\\n    ELSE 0 \\\\\\\\\\\\n   END AS is_rush, \\\\\\\\\\\\n  CASE \\\\\\\\\\\\n   WHEN weekday >5 THEN 1\\\\\\\\\\\\n   ELSE 0\\\\\\\\\\\\n  END AS is_weekend, \\\\\\\\\\\\nis_holiday, level, label FROM __THIS__\\\\\")\\\\n\\\\ndataTrans = sqlTrans.transform(data)\\\\ndataTrans.show()\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:15:54+0000\",\"config\":{\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"colWidth\":12,\"editorMode\":\"ace/mode/python\",\"fontSize\":9,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"TEXT\",\"data\":\"+-------------------+------------------+-------------------+--------------------+---------+---------+-------------------+------------------+--------------------+--------------------+--------------------+-------------------+--------------------+-------------------+-------+----------+----------+-----+-----+\\\\n|         location_x|        location_y|        sin_weekday|         cos_weekday|sin_month|cos_month|            sin_day|           cos_day|            sin_hour|            cos_hour|             sin_min|            cos_min|             sin_sec|            cos_sec|is_rush|is_weekend|is_holiday|level|label|\\\\n+-------------------+------------------+-------------------+--------------------+---------+---------+-------------------+------------------+--------------------+--------------------+--------------------+-------------------+--------------------+-------------------+-------+----------+----------+-----+-----+\\\\n|-118.07918500000001|34.148016999999996|0.43388373911755823|  -0.900968867902419|      0.0|      1.0| 0.5712682150947923|0.8207634412072763|  0.5000000000000003| -0.8660254037844385| -0.9510565162951536|0.30901699437494723|  0.5000000000000003|-0.8660254037844385|      0|         0|         0|    2|    0|\\\\n|        -118.082976|         33.774503|                0.0|                 1.0|      0.0|      1.0| 0.9884683243281114|0.1514277775045767|  0.7071067811865476| -0.7071067811865475|  0.6691306063588581| 0.7431448254773944|1.224646799147353...|               -1.0|      0|         0|         0|    3|    1|\\\\n|-118.08305700000001|         34.147963| 0.7818314824680298|  0.6234898018587336|      0.0|      1.0|0.20129852008866006|0.9795299412524945|-0.25881904510252035| -0.9659258262890684|  0.8660254037844386| 0.5000000000000001| -0.6691306063588588| 0.7431448254773937|      0|         0|         0|    2|    0|\\\\n|-118.08373600000002|         34.148038| 0.9749279121818236|-0.22252093395631434|      0.0|      1.0|0.39435585511331855|0.9189578116202306| -0.9659258262890681| -0.2588190451025215|-0.20791169081775987| 0.9781476007338056|  0.5877852522924731| 0.8090169943749475|      1|         0|         0|    2|    0|\\\\n|        -118.083992|         33.774538|                0.0|                 1.0|      0.0|      1.0| 0.9884683243281114|0.1514277775045767|  0.8660254037844387| -0.4999999999999998| -0.3090169943749476| 0.9510565162951535| -0.9945218953682734|0.10452846326765299|      1|         0|         0|    3|    1|\\\\n|        -118.084849|         34.148274| 0.7818314824680298|  0.6234898018587336|      0.0|      1.0|0.20129852008866006|0.9795299412524945|-0.25881904510252035| -0.9659258262890684|  0.8660254037844386| 0.5000000000000001| -0.6691306063588588| 0.7431448254773937|      0|         0|         0|    2|    0|\\\\n|        -118.084849|         34.148274| 0.7818314824680298|  0.6234898018587336|      0.0|      1.0|0.20129852008866006|0.9795299412524945|-0.25881904510252035| -0.9659258262890684|  0.8660254037844386| 0.5000000000000001| -0.6691306063588588| 0.7431448254773937|      0|         0|         0|    2|    0|\\\\n|        -118.084849|         34.148274| 0.7818314824680298|  0.6234898018587336|      0.0|      1.0|0.20129852008866006|0.9795299412524945|-0.25881904510252035| -0.9659258262890684|  0.8660254037844386| 0.5000000000000001| -0.6691306063588588| 0.7431448254773937|      0|         0|         0|    3|    1|\\\\n|        -118.084849|         34.148274| 0.7818314824680298|  0.6234898018587336|      0.0|      1.0|0.20129852008866006|0.9795299412524945|-0.25881904510252035| -0.9659258262890684|  0.8660254037844386| 0.5000000000000001| -0.6691306063588588| 0.7431448254773937|      0|         0|         0|    3|    1|\\\\n|        -118.084849|         34.148274| 0.9749279121818236|-0.22252093395631434|      0.0|      1.0|0.39435585511331855|0.9189578116202306| -0.7071067811865471| -0.7071067811865479| -0.7431448254773946| 0.6691306063588578| -0.5000000000000004| 0.8660254037844384|      1|         0|         0|    2|    0|\\\\n|        -118.084849|         34.148274| 0.9749279121818236|-0.22252093395631434|      0.0|      1.0|0.39435585511331855|0.9189578116202306| -0.7071067811865471| -0.7071067811865479| -0.7431448254773946| 0.6691306063588578| -0.5000000000000004| 0.8660254037844384|      1|         0|         0|    3|    1|\\\\n|        -118.084849|         34.148274|0.43388373911755823|  -0.900968867902419|      0.0|      1.0| 0.5712682150947923|0.8207634412072763|  0.5000000000000003| -0.8660254037844385| -0.9510565162951536|0.30901699437494723|  0.5000000000000003|-0.8660254037844385|      0|         0|         0|    2|    0|\\\\n|        -118.084849|         34.148274| -0.433883739117558| -0.9009688679024191|      0.0|      1.0|   0.72479278722912|0.6889669190756866| -0.4999999999999997| -0.8660254037844388| 0.40673664307580043|-0.9135454576426008|  0.9510565162951535|0.30901699437494745|      0|         0|         0|    3|    1|\\\\n|-118.08511100000001|         33.774715|                0.0|                 1.0|      0.0|      1.0| 0.9884683243281114|0.1514277775045767|  0.8660254037844387| -0.4999999999999998| -0.3090169943749476| 0.9510565162951535| -0.9945218953682734|0.10452846326765299|      1|         0|         0|    3|    1|\\\\n|-118.08523999999998|34.148396999999996|0.43388373911755823|  -0.900968867902419|      0.0|      1.0| 0.5712682150947923|0.8207634412072763|  0.9659258262890683|-0.25881904510252063| 0.20791169081775931| 0.9781476007338057|  0.6691306063588583|-0.7431448254773941|      1|         0|         0|    2|    0|\\\\n|-118.08523999999998|34.148396999999996| -0.433883739117558| -0.9009688679024191|      0.0|      1.0|   0.72479278722912|0.6889669190756866|                -1.0|-1.83697019872102...|-0.10452846326765305|-0.9945218953682734|-0.10452846326765305|-0.9945218953682734|      0|         0|         0|    2|    0|\\\\n|-118.08548700000001|         33.774813|                0.0|                 1.0|      0.0|      1.0| 0.9884683243281114|0.1514277775045767|  0.7071067811865476| -0.7071067811865475|  0.6691306063588581| 0.7431448254773944|1.224646799147353...|               -1.0|      0|         0|         0|    3|    1|\\\\n|        -118.085629|34.148534000000005| 0.7818314824680298|  0.6234898018587336|      0.0|      1.0|0.20129852008866006|0.9795299412524945|-0.25881904510252035| -0.9659258262890684|  0.8660254037844386| 0.5000000000000001| -0.6691306063588588| 0.7431448254773937|      0|         0|         0|    2|    0|\\\\n|        -118.085629|34.148534000000005| 0.7818314824680298|  0.6234898018587336|      0.0|      1.0|0.20129852008866006|0.9795299412524945|-0.25881904510252035| -0.9659258262890684|  0.8660254037844386| 0.5000000000000001| -0.6691306063588588| 0.7431448254773937|      0|         0|         0|    2|    0|\\\\n|        -118.085629|34.148534000000005|0.43388373911755823|  -0.900968867902419|      0.0|      1.0| 0.5712682150947923|0.8207634412072763|  0.5000000000000003| -0.8660254037844385| -0.9510565162951536|0.30901699437494723|  0.5000000000000003|-0.8660254037844385|      0|         0|         0|    2|    0|\\\\n+-------------------+------------------+-------------------+--------------------+---------+---------+-------------------+------------------+--------------------+--------------------+--------------------+-------------------+--------------------+-------------------+-------+----------+----------+-----+-----+\\\\nonly showing top 20 rows\\\\n\\\\n\"}]},\"runtimeInfos\":{\"jobUrl\":{\"propertyName\":\"jobUrl\",\"label\":\"SPARK JOB\",\"tooltip\":\"View in Spark web UI\",\"group\":\"spark\",\"values\":[\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=471\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=472\"],\"interpreterSettingId\":\"spark\"}},\"apps\":[],\"jobName\":\"paragraph_1570433237004_-1756040920\",\"id\":\"20190624-103435_576179035\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:15:54+0000\",\"dateFinished\":\"2019-10-07T09:15:54+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:191\"},{\"text\":\"%md\\\\n\\\\n## Split data for training and testing\\\\nIt is common practice when building supervised machine learning models to split the source data, using some of it to train the model and reserving some to test the trained model. In this exercise, you will use 75% of the data for training, and reserve 25% for testing.\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:15:54+0000\",\"config\":{\"tableHide\":false,\"editorSetting\":{\"language\":\"markdown\",\"editOnDblClick\":true,\"completionKey\":\"TAB\",\"completionSupport\":false},\"colWidth\":12,\"editorMode\":\"ace/mode/markdown\",\"fontSize\":9,\"editorHide\":true,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"HTML\",\"data\":\"<div class=\\\\\"markdown-body\\\\\">\\\\n<h2>Split data for training and testing</h2>\\\\n<p>It is common practice when building supervised machine learning models to split the source data, using some of it to train the model and reserving some to test the trained model. In this exercise, you will use 75% of the data for training, and reserve 25% for testing.</p>\\\\n</div>\"}]},\"apps\":[],\"jobName\":\"paragraph_1570433237005_814495984\",\"id\":\"20190624-103453_1898833614\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:15:54+0000\",\"dateFinished\":\"2019-10-07T09:15:54+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:192\"},{\"text\":\"%spark.pyspark\\\\n\\\\nfrom pyspark.storagelevel import StorageLevel\\\\n\\\\n# Split the data\\\\nsplits = dataTrans.randomSplit([0.75, 0.25], seed=1234)\\\\ntrain = splits[0]\\\\ntest = splits[1].withColumnRenamed(\\\\\"label\\\\\", \\\\\"trueLabel\\\\\")\\\\n#train.persist(StorageLevel.MEMORY_AND_DISK)\\\\n#test.persist(StorageLevel.MEMORY_AND_DISK)\\\\ntrain.persist(StorageLevel.DISK_ONLY)\\\\ntest.persist(StorageLevel.DISK_ONLY)\\\\ntrain.show()\\\\n#train.cache()\\\\n#test.cache()\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:15:55+0000\",\"config\":{\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"colWidth\":12,\"editorMode\":\"ace/mode/python\",\"fontSize\":9,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"TEXT\",\"data\":\"+-------------------+----------+-------------------+--------------------+---------+---------+-------------------+------------------+--------------------+-------------------+--------------------+--------------------+-------------------+--------------------+-------+----------+----------+-----+-----+\\\\n|         location_x|location_y|        sin_weekday|         cos_weekday|sin_month|cos_month|            sin_day|           cos_day|            sin_hour|           cos_hour|             sin_min|             cos_min|            sin_sec|             cos_sec|is_rush|is_weekend|is_holiday|level|label|\\\\n+-------------------+----------+-------------------+--------------------+---------+---------+-------------------+------------------+--------------------+-------------------+--------------------+--------------------+-------------------+--------------------+-------+----------+----------+-----+-----+\\\\n|        -118.359154| 33.952387|                0.0|                 1.0|      0.0|      1.0|                0.0|               1.0|  0.7071067811865476|-0.7071067811865475|                 0.0|                 1.0|0.40673664307580043| -0.9135454576426008|      1|         0|         1|    3|    1|\\\\n|        -118.359154| 33.952387| 0.9749279121818236|-0.22252093395631434|      0.0|      1.0|0.39435585511331855|0.9189578116202306| -0.8660254037844385|-0.5000000000000004| 0.10452846326765373| -0.9945218953682733| -0.866025403784439| 0.49999999999999933|      1|         0|         0|    2|    0|\\\\n|        -118.359154|   34.1343| -0.433883739117558| -0.9009688679024191|      0.0|      1.0|   0.72479278722912|0.6889669190756866|  0.5000000000000003|-0.8660254037844385| -0.9945218953682733|-0.10452846326765423|0.40673664307580043| -0.9135454576426008|      0|         0|         0|    1|    0|\\\\n|        -118.359154|   34.1343|                0.0|                 1.0|      0.0|      1.0|                0.0|               1.0| 0.49999999999999994| 0.8660254037844387|  0.9945218953682734|-0.10452846326765333|-0.9945218953682734| 0.10452846326765299|      0|         0|         1|    1|    0|\\\\n|        -118.359154|   34.1343|                0.0|                 1.0|      0.0|      1.0| 0.9884683243281114|0.1514277775045767|-0.25881904510252035|-0.9659258262890684| 0.10452846326765373| -0.9945218953682733| 0.9945218953682733| 0.10452846326765368|      0|         0|         0|    3|    1|\\\\n|        -118.359154|   34.1343|                0.0|                 1.0|      0.0|      1.0| 0.9884683243281114|0.1514277775045767|1.224646799147353...|               -1.0|  0.5000000000000003| -0.8660254037844385|-0.9781476007338058| 0.20791169081775857|      0|         0|         0|    4|    1|\\\\n|        -118.359144| 33.837584|-0.9749279121818236| -0.2225209339563146|      0.0|      1.0| 0.8486442574947509|0.5289640103269624|-0.25881904510252035|-0.9659258262890684| 0.49999999999999994|  0.8660254037844387|-0.9781476007338056| -0.2079116908177598|      0|         0|         0|    4|    1|\\\\n|         -118.35914| 34.154007|                0.0|                 1.0|      0.0|      1.0|                0.0|               1.0|  0.8660254037844387|-0.4999999999999998|  0.9945218953682734|-0.10452846326765333|-0.9510565162951535|-0.30901699437494756|      1|         0|         1|    3|    1|\\\\n|-118.35913799999999| 33.945457| -0.433883739117558| -0.9009688679024191|      0.0|      1.0|   0.72479278722912|0.6889669190756866|   0.258819045102521|-0.9659258262890682| 0.10452846326765346|  0.9945218953682733|0.40673664307580015|  0.9135454576426009|      0|         0|         0|    3|    1|\\\\n|-118.35913500000001| 33.920048|-0.9749279121818236| -0.2225209339563146|      0.0|      1.0| 0.8486442574947509|0.5289640103269624| -0.9659258262890681|-0.2588190451025215| -0.6691306063588579| -0.7431448254773945|-0.9510565162951535|-0.30901699437494756|      1|         0|         0|    2|    0|\\\\n|-118.35913500000001| 33.920048|                0.0|                 1.0|      0.0|      1.0| 0.9884683243281114|0.1514277775045767| -0.4999999999999997|-0.8660254037844388| -0.9135454576426011| 0.40673664307579976| 0.6691306063588581|  0.7431448254773944|      0|         0|         0|    2|    0|\\\\n|-118.35913500000001| 34.069024|                0.0|                 1.0|      0.0|      1.0| 0.9884683243281114|0.1514277775045767|  0.8660254037844387|-0.4999999999999998|  0.8660254037844387| -0.4999999999999998|               -1.0|-1.83697019872102...|      1|         0|         0|    3|    1|\\\\n|        -118.359131| 34.090819|-0.9749279121818236| -0.2225209339563146|      0.0|      1.0| 0.8486442574947509|0.5289640103269624| -0.4999999999999997|-0.8660254037844388|  0.8660254037844387| -0.4999999999999998|0.49999999999999994|  0.8660254037844387|      0|         0|         0|    3|    1|\\\\n|        -118.359131| 34.090819| -0.433883739117558| -0.9009688679024191|      0.0|      1.0|   0.72479278722912|0.6889669190756866| -0.4999999999999997|-0.8660254037844388|1.224646799147353...|                -1.0| 0.8090169943749475|  -0.587785252292473|      0|         0|         0|    3|    1|\\\\n|        -118.359131| 34.090819|                0.0|                 1.0|      0.0|      1.0| 0.9884683243281114|0.1514277775045767|-0.25881904510252035|-0.9659258262890684|  0.5877852522924731|  0.8090169943749475|-0.9781476007338056| -0.2079116908177598|      0|         0|         0|    3|    1|\\\\n|        -118.359131| 34.090819|0.43388373911755823|  -0.900968867902419|      0.0|      1.0| 0.5712682150947923|0.8207634412072763|  0.5000000000000003|-0.8660254037844385| -0.3090169943749473| -0.9510565162951536| 0.9510565162951535| 0.30901699437494745|      0|         0|         0|    4|    1|\\\\n|        -118.359131| 34.090819|0.43388373911755823|  -0.900968867902419|      0.0|      1.0| 0.5712682150947923|0.8207634412072763|  0.7071067811865476|-0.7071067811865475|  0.8660254037844386|  0.5000000000000001|                0.0|                 1.0|      0|         0|         0|    3|    1|\\\\n|        -118.359131| 34.090819|0.43388373911755823|  -0.900968867902419|      0.0|      1.0| 0.5712682150947923|0.8207634412072763|  0.7071067811865476|-0.7071067811865475|  0.8660254037844386|  0.5000000000000001|                0.0|                 1.0|      0|         0|         0|    3|    1|\\\\n|        -118.359131| 34.090819|0.43388373911755823|  -0.900968867902419|      0.0|      1.0| 0.5712682150947923|0.8207634412072763|  0.7071067811865476|-0.7071067811865475|  0.8660254037844386|  0.5000000000000001|                0.0|                 1.0|      0|         0|         0|    4|    1|\\\\n|        -118.359131| 34.090819| 0.9749279121818236|-0.22252093395631434|      0.0|      1.0|0.39435585511331855|0.9189578116202306| -0.7071067811865477| 0.7071067811865474| -0.9945218953682733|-0.10452846326765423| 0.6691306063588583| -0.7431448254773941|      0|         0|         0|    2|    0|\\\\n+-------------------+----------+-------------------+--------------------+---------+---------+-------------------+------------------+--------------------+-------------------+--------------------+--------------------+-------------------+--------------------+-------+----------+----------+-----+-----+\\\\nonly showing top 20 rows\\\\n\\\\n\"}]},\"runtimeInfos\":{\"jobUrl\":{\"propertyName\":\"jobUrl\",\"label\":\"SPARK JOB\",\"tooltip\":\"View in Spark web UI\",\"group\":\"spark\",\"values\":[\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=473\"],\"interpreterSettingId\":\"spark\"}},\"apps\":[],\"jobName\":\"paragraph_1570433237005_139191504\",\"id\":\"20190624-103532_729046378\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:15:55+0000\",\"dateFinished\":\"2019-10-07T09:15:55+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:193\"},{\"text\":\"%md\\\\n\\\\n### Let\\'s see if dataset is unbalanced\\\\n\\\\nCounting how many 1\\'s and 0\\'s for label column to see if dataset is unbalanced. \\\\n\\\\nAlso let\\'s calculate number and percentage of 1\\'s in the training set.\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:15:55+0000\",\"config\":{\"tableHide\":false,\"editorSetting\":{\"language\":\"markdown\",\"editOnDblClick\":true,\"completionKey\":\"TAB\",\"completionSupport\":false},\"colWidth\":12,\"editorMode\":\"ace/mode/markdown\",\"fontSize\":9,\"editorHide\":true,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"HTML\",\"data\":\"<div class=\\\\\"markdown-body\\\\\">\\\\n<h3>Let&rsquo;s see if dataset is unbalanced</h3>\\\\n<p>Counting how many 1&rsquo;s and 0&rsquo;s for label column to see if dataset is unbalanced. </p>\\\\n<p>Also let&rsquo;s calculate number and percentage of 1&rsquo;s in the training set.</p>\\\\n</div>\"}]},\"apps\":[],\"jobName\":\"paragraph_1570433237005_1656279613\",\"id\":\"20190624-103602_1959679366\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:15:55+0000\",\"dateFinished\":\"2019-10-07T09:15:55+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:194\"},{\"text\":\"%spark.pyspark\\\\n\\\\n#Let\\'s see if dataset is unbalanced\\\\ndf = data.groupBy(\\'label\\').count().show()\\\\n\\\\n#let\\'s see the imbalance in train dataset\\\\ndataset_size=float(train.select(\\\\\"label\\\\\").count())\\\\nnumPositives=train.select(\\\\\"label\\\\\").where(\\'label == 1\\').count()\\\\nper_ones=(float(numPositives)/float(dataset_size))*100\\\\nnumNegatives=float(dataset_size-numPositives)\\\\nprint(\\'The number of ones are {}\\'.format(numPositives))\\\\nprint(\\'Percentage of ones are {}\\'.format(per_ones))\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:15:55+0000\",\"config\":{\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"colWidth\":12,\"editorMode\":\"ace/mode/python\",\"fontSize\":9,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"TEXT\",\"data\":\"+-----+-------+\\\\n|label|  count|\\\\n+-----+-------+\\\\n|    1|8795159|\\\\n|    0|4523109|\\\\n+-----+-------+\\\\n\\\\nThe number of ones are 6596088\\\\nPercentage of ones are 66.0331731179\\\\n\"}]},\"runtimeInfos\":{\"jobUrl\":{\"propertyName\":\"jobUrl\",\"label\":\"SPARK JOB\",\"tooltip\":\"View in Spark web UI\",\"group\":\"spark\",\"values\":[\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=474\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=475\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=476\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=477\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=478\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=479\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=480\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=481\"],\"interpreterSettingId\":\"spark\"}},\"apps\":[],\"jobName\":\"paragraph_1570433237006_-373079458\",\"id\":\"20190624-103625_153191632\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:15:55+0000\",\"dateFinished\":\"2019-10-07T09:16:02+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:195\"},{\"text\":\"%md\\\\n\\\\n## Define the Pipeline\\\\n### Prepare the Training Data\\\\nTo train the classification model, you need a training data set that includes a vector of numeric features, and a label column. In this exercise, you will use the **VectorAssembler** class to transform the feature columns into a vector. But before we will normalize numerical values using **minMax**.\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:16:02+0000\",\"config\":{\"tableHide\":false,\"editorSetting\":{\"language\":\"markdown\",\"editOnDblClick\":true,\"completionKey\":\"TAB\",\"completionSupport\":false},\"colWidth\":12,\"editorMode\":\"ace/mode/markdown\",\"fontSize\":9,\"editorHide\":true,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"HTML\",\"data\":\"<div class=\\\\\"markdown-body\\\\\">\\\\n<h2>Define the Pipeline</h2>\\\\n<h3>Prepare the Training Data</h3>\\\\n<p>To train the classification model, you need a training data set that includes a vector of numeric features, and a label column. In this exercise, you will use the <strong>VectorAssembler</strong> class to transform the feature columns into a vector. But before we will normalize numerical values using <strong>minMax</strong>.</p>\\\\n</div>\"}]},\"apps\":[],\"jobName\":\"paragraph_1570433237006_451060042\",\"id\":\"20190624-103642_1866260039\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:16:02+0000\",\"dateFinished\":\"2019-10-07T09:16:02+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:196\"},{\"text\":\"%spark.pyspark\\\\n\\\\n# Vector is normalized\\\\nnumVect = VectorAssembler(inputCols = [\\\\\"location_x\\\\\", \\\\\"location_y\\\\\", \\\\\"sin_weekday\\\\\",\\\\\"cos_weekday\\\\\", \\\\\"sin_day\\\\\", \\\\\"cos_day\\\\\", \\\\\"sin_hour\\\\\", \\\\\"cos_hour\\\\\",\\\\\"sin_min\\\\\",\\\\\"cos_min\\\\\", \\\\\"sin_sec\\\\\",\\\\\"cos_sec\\\\\", \\\\\"sin_weekday\\\\\", \\\\\"cos_weekday\\\\\",], outputCol=\\\\\"numFeatures\\\\\")\\\\n#Norm = Normalizer(inputCol = numVect.getOutputCol(), outputCol=\\\\\"normFeatures\\\\\", p=1.0)\\\\nminMax = MinMaxScaler(inputCol = numVect.getOutputCol(), outputCol=\\\\\"normFeatures\\\\\")\\\\n\\\\n# the following columns are categorical and should be Category features\\\\ncatVect = VectorAssembler(inputCols = [\\\\\"is_weekend\\\\\", \\\\\"is_rush\\\\\"], outputCol=\\\\\"catFeatures\\\\\")\\\\ncatIdx = VectorIndexer(inputCol = catVect.getOutputCol(), outputCol = \\\\\"idxCatFeatures\\\\\")\\\\n\\\\n# creating vector column - features\\\\nfeatVect = VectorAssembler(inputCols=[\\\\\"idxCatFeatures\\\\\", \\\\\"normFeatures\\\\\"], outputCol=\\\\\"features\\\\\")\\\\n#featVect = VectorAssembler(inputCols=[\\\\\"is_weekend\\\\\", \\\\\"is_rush\\\\\", \\\\\"normFeatures\\\\\"], outputCol=\\\\\"features\\\\\")\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:16:02+0000\",\"config\":{\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"colWidth\":12,\"editorMode\":\"ace/mode/python\",\"fontSize\":9,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[]},\"apps\":[],\"jobName\":\"paragraph_1570433237006_470587397\",\"id\":\"20190624-103708_1592499893\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:16:02+0000\",\"dateFinished\":\"2019-10-07T09:16:02+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:197\"},{\"text\":\"\\\\n%md\\\\n\\\\n#### Pipeline\\\\nPipeline process the series of transformation above. \\\\n\\\\n### Train a Classification Model\\\\nNext, we need to train a classification model using the training data. To do this, create an instance of the classification algorithm we want to use and use its **fit** method to train a model based on the training DataFrame. In this case, we will use a *RandomForest* classification algorithm.\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:16:02+0000\",\"config\":{\"tableHide\":false,\"editorSetting\":{\"language\":\"markdown\",\"editOnDblClick\":true,\"completionKey\":\"TAB\",\"completionSupport\":false},\"colWidth\":12,\"editorMode\":\"ace/mode/markdown\",\"fontSize\":9,\"editorHide\":true,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"HTML\",\"data\":\"<div class=\\\\\"markdown-body\\\\\">\\\\n<h4>Pipeline</h4>\\\\n<p>Pipeline process the series of transformation above. </p>\\\\n<h3>Train a Classification Model</h3>\\\\n<p>Next, we need to train a classification model using the training data. To do this, create an instance of the classification algorithm we want to use and use its <strong>fit</strong> method to train a model based on the training DataFrame. In this case, we will use a <em>RandomForest</em> classification algorithm.</p>\\\\n</div>\"}]},\"apps\":[],\"jobName\":\"paragraph_1570433237007_-2078868783\",\"id\":\"20190624-103730_1758370622\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:16:02+0000\",\"dateFinished\":\"2019-10-07T09:16:02+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:198\"},{\"text\":\"%spark.pyspark\\\\n\\\\n# model\\\\n#dt = DecisionTreeClassifier(labelCol=\\\\\"label\\\\\", featuresCol=\\\\\"features\\\\\", maxDepth = 30)\\\\nrf = RandomForestClassifier(featuresCol = \\'features\\', labelCol = \\'label\\')\\\\n# pipeline\\\\npipeline = Pipeline(stages=[numVect, minMax, catVect, catIdx, featVect])\\\\nfittedPipeline = pipeline.fit(train)\\\\ntransformedTraining = fittedPipeline.transform(train)\\\\n\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:16:02+0000\",\"config\":{\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"colWidth\":12,\"editorMode\":\"ace/mode/python\",\"fontSize\":9,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[]},\"runtimeInfos\":{\"jobUrl\":{\"propertyName\":\"jobUrl\",\"label\":\"SPARK JOB\",\"tooltip\":\"View in Spark web UI\",\"group\":\"spark\",\"values\":[\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=482\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=483\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=484\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=485\"],\"interpreterSettingId\":\"spark\"}},\"apps\":[],\"jobName\":\"paragraph_1570433237007_1065604620\",\"id\":\"20190624-103753_1486917983\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:16:02+0000\",\"dateFinished\":\"2019-10-07T09:16:09+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:199\"},{\"text\":\"%spark.pyspark\\\\n\\\\n# transformedTraining.persist()\\\\ntransformedTraining.persist(StorageLevel.MEMORY_AND_DISK)\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:16:09+0000\",\"config\":{\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"colWidth\":12,\"editorMode\":\"ace/mode/python\",\"fontSize\":9,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"TEXT\",\"data\":\"DataFrame[location_x: double, location_y: double, sin_weekday: double, cos_weekday: double, sin_month: double, cos_month: double, sin_day: double, cos_day: double, sin_hour: double, cos_hour: double, sin_min: double, cos_min: double, sin_sec: double, cos_sec: double, is_rush: int, is_weekend: int, is_holiday: int, level: int, label: int, numFeatures: vector, normFeatures: vector, catFeatures: vector, idxCatFeatures: vector, features: vector]\\\\n\"}]},\"apps\":[],\"jobName\":\"paragraph_1570433237007_-1750450620\",\"id\":\"20190814-025039_603347498\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:16:09+0000\",\"dateFinished\":\"2019-10-07T09:16:09+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:200\"},{\"text\":\"%md\\\\n\\\\n### Tuning Parameters\\\\n**TrainValidationSplit** is used to evaluate each combination of parameters defined in a **ParameterGrid**, since TVS is very fast. We are tuning the parameter **numTrees** - number of trees in the forest and **maxDepth** - the maximum depth of each tree. This step will take some more time to compute. \\\\n\\\\nNote: We will use later Cross Validation to test 2nd model\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:16:09+0000\",\"config\":{\"tableHide\":false,\"editorSetting\":{\"language\":\"markdown\",\"editOnDblClick\":true,\"completionKey\":\"TAB\",\"completionSupport\":false},\"colWidth\":12,\"editorMode\":\"ace/mode/markdown\",\"fontSize\":9,\"editorHide\":true,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"HTML\",\"data\":\"<div class=\\\\\"markdown-body\\\\\">\\\\n<h3>Tuning Parameters</h3>\\\\n<p><strong>TrainValidationSplit</strong> is used to evaluate each combination of parameters defined in a <strong>ParameterGrid</strong>, since TVS is very fast. We are tuning the parameter <strong>numTrees</strong> - number of trees in the forest and <strong>maxDepth</strong> - the maximum depth of each tree. This step will take some more time to compute. </p>\\\\n<p>Note: We will use later Cross Validation to test 2nd model</p>\\\\n</div>\"}]},\"apps\":[],\"jobName\":\"paragraph_1570433237008_289826646\",\"id\":\"20190624-103814_1604941903\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:16:09+0000\",\"dateFinished\":\"2019-10-07T09:16:09+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:201\"},{\"text\":\"%spark.pyspark\\\\n\\\\n# params to tune RF\\\\n# params to tune RF\\\\nparamGrid = (ParamGridBuilder()\\\\n             .addGrid(rf.numTrees, [5])\\\\\\\\\\\\n#             .addGrid(rf.numTrees, [7])\\\\\\\\\\\\n#             .addGrid(rf.numTrees, [5,7])\\\\\\\\\\\\n             .addGrid(rf.maxDepth, [20])\\\\\\\\\\\\n#             .addGrid(rf.maxDepth, [30])\\\\\\\\\\\\n#             .addGrid(rf.maxDepth, [20, 30])\\\\\\\\\\\\n             .build())\\\\n\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:16:09+0000\",\"config\":{\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"colWidth\":12,\"editorMode\":\"ace/mode/python\",\"fontSize\":9,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[]},\"apps\":[],\"jobName\":\"paragraph_1570433237008_1650312349\",\"id\":\"20190813-081447_219823\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:16:09+0000\",\"dateFinished\":\"2019-10-07T09:16:09+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:202\"},{\"text\":\"%spark.pyspark\\\\ntvs = TrainValidationSplit(estimator=rf, evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid, trainRatio=0.7)\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:16:09+0000\",\"config\":{\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"colWidth\":12,\"editorMode\":\"ace/mode/python\",\"fontSize\":9,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[]},\"apps\":[],\"jobName\":\"paragraph_1570433237008_1140289465\",\"id\":\"20190813-081508_195278163\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:16:09+0000\",\"dateFinished\":\"2019-10-07T09:16:09+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:203\"},{\"text\":\"%spark.pyspark\\\\n\\\\n# the first best model\\\\n#model = tvs.fit(train)\\\\nTVSmodel = tvs.fit(transformedTraining)\\\\n#model = pipeline.fit(train)\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:16:09+0000\",\"config\":{\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"colWidth\":12,\"editorMode\":\"ace/mode/python\",\"fontSize\":9,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[]},\"runtimeInfos\":{\"jobUrl\":{\"propertyName\":\"jobUrl\",\"label\":\"SPARK JOB\",\"tooltip\":\"View in Spark web UI\",\"group\":\"spark\",\"values\":[\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=486\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=487\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=488\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=489\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=490\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=491\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=492\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=493\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=494\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=495\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=496\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=497\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=498\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=499\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=500\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=501\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=502\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=503\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=504\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=505\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=506\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=507\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=508\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=509\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=510\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=511\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=512\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=513\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=514\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=515\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=516\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=517\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=518\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=519\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=520\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=521\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=522\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=523\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=524\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=525\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=526\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=527\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=528\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=529\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=530\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=531\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=532\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=533\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=534\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=535\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=536\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=537\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=538\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=539\"],\"interpreterSettingId\":\"spark\"}},\"apps\":[],\"jobName\":\"paragraph_1570433237009_1380749312\",\"id\":\"20190624-103833_289001770\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:16:09+0000\",\"dateFinished\":\"2019-10-07T09:20:23+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:204\"},{\"text\":\"%md\\\\n\\\\n## Testing the Model\\\\nNow we\\'re ready to use the **transform** method of the model to generate some predictions. We can use this approach to predict jam where the label is unknown; but in this case we are using the test data which includes a known true label value, so we can compare the predicted jam to the actual jam. \",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:20:23+0000\",\"config\":{\"tableHide\":false,\"editorSetting\":{\"language\":\"markdown\",\"editOnDblClick\":true,\"completionKey\":\"TAB\",\"completionSupport\":false},\"colWidth\":12,\"editorMode\":\"ace/mode/markdown\",\"fontSize\":9,\"editorHide\":true,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"HTML\",\"data\":\"<div class=\\\\\"markdown-body\\\\\">\\\\n<h2>Testing the Model</h2>\\\\n<p>Now we&rsquo;re ready to use the <strong>transform</strong> method of the model to generate some predictions. We can use this approach to predict jam where the label is unknown; but in this case we are using the test data which includes a known true label value, so we can compare the predicted jam to the actual jam.</p>\\\\n</div>\"}]},\"apps\":[],\"jobName\":\"paragraph_1570433237009_-899093140\",\"id\":\"20190624-103850_1195090378\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:20:23+0000\",\"dateFinished\":\"2019-10-07T09:20:23+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:205\"},{\"text\":\"%spark.pyspark\\\\n\\\\nfittedTest = pipeline.fit(test)\\\\ntransformedTest = fittedPipeline.transform(test)\\\\n#transformedTest.persist()\\\\ntransformedTest.persist(StorageLevel.MEMORY_AND_DISK)\\\\n\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:20:23+0000\",\"config\":{\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"colWidth\":12,\"editorMode\":\"ace/mode/python\",\"fontSize\":9,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"TEXT\",\"data\":\"DataFrame[location_x: double, location_y: double, sin_weekday: double, cos_weekday: double, sin_month: double, cos_month: double, sin_day: double, cos_day: double, sin_hour: double, cos_hour: double, sin_min: double, cos_min: double, sin_sec: double, cos_sec: double, is_rush: int, is_weekend: int, is_holiday: int, level: int, trueLabel: int, numFeatures: vector, normFeatures: vector, catFeatures: vector, idxCatFeatures: vector, features: vector]\\\\n\"}]},\"runtimeInfos\":{\"jobUrl\":{\"propertyName\":\"jobUrl\",\"label\":\"SPARK JOB\",\"tooltip\":\"View in Spark web UI\",\"group\":\"spark\",\"values\":[\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=540\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=541\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=542\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=543\"],\"interpreterSettingId\":\"spark\"}},\"apps\":[],\"jobName\":\"paragraph_1570433237009_-486351487\",\"id\":\"20190813-081534_2023622316\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:20:23+0000\",\"dateFinished\":\"2019-10-07T09:20:26+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:206\"},{\"text\":\"%spark.pyspark\\\\n\\\\nprediction = TVSmodel.transform(transformedTest)\\\\nprediction.persist(StorageLevel.MEMORY_AND_DISK)\\\\n\\\\n\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:20:26+0000\",\"config\":{\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"colWidth\":12,\"editorMode\":\"ace/mode/python\",\"fontSize\":9,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"TEXT\",\"data\":\"DataFrame[location_x: double, location_y: double, sin_weekday: double, cos_weekday: double, sin_month: double, cos_month: double, sin_day: double, cos_day: double, sin_hour: double, cos_hour: double, sin_min: double, cos_min: double, sin_sec: double, cos_sec: double, is_rush: int, is_weekend: int, is_holiday: int, level: int, trueLabel: int, numFeatures: vector, normFeatures: vector, catFeatures: vector, idxCatFeatures: vector, features: vector, rawPrediction: vector, probability: vector, prediction: double]\\\\n\"}]},\"apps\":[],\"jobName\":\"paragraph_1570433237010_1449651813\",\"id\":\"20190624-103908_801997968\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:20:27+0000\",\"dateFinished\":\"2019-10-07T09:20:27+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:207\"},{\"text\":\"%spark.pyspark\\\\n\\\\npredicted = prediction.select(\\\\\"features\\\\\", \\\\\"prediction\\\\\", \\\\\"probability\\\\\", \\\\\"trueLabel\\\\\")\\\\npredicted.show(100)\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:20:27+0000\",\"config\":{\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"colWidth\":12,\"editorMode\":\"ace/mode/python\",\"fontSize\":9,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"TEXT\",\"data\":\"+--------------------+----------+--------------------+---------+\\\\n|            features|prediction|         probability|trueLabel|\\\\n+--------------------+----------+--------------------+---------+\\\\n|[0.0,0.0,0.543711...|       1.0|[0.19682908412707...|        1|\\\\n|[0.0,0.0,0.543714...|       1.0|[0.30372873591197...|        1|\\\\n|[1.0,0.0,0.543719...|       1.0|[0.34441877494528...|        1|\\\\n|[0.0,0.0,0.543749...|       0.0|[0.61656786783814...|        0|\\\\n|[0.0,0.0,0.543766...|       1.0|[0.12154818765592...|        1|\\\\n|[1.0,0.0,0.543767...|       0.0|[0.73743051882772...|        0|\\\\n|[0.0,0.0,0.543767...|       1.0|[0.39105247678967...|        0|\\\\n|[0.0,1.0,0.543772...|       1.0|[0.18081019427021...|        1|\\\\n|[0.0,1.0,0.543778...|       1.0|[0.14616887010758...|        1|\\\\n|[0.0,0.0,0.543778...|       1.0|[0.18019906021068...|        1|\\\\n|[0.0,0.0,0.543780...|       0.0|[0.75315324234508...|        0|\\\\n|[0.0,0.0,0.543780...|       0.0|[0.51572997379587...|        0|\\\\n|[0.0,1.0,0.543790...|       1.0|[0.12918485748462...|        1|\\\\n|[0.0,1.0,0.543835...|       1.0|[0.05990719170346...|        1|\\\\n|[0.0,0.0,0.543840...|       1.0|[0.28695159811187...|        1|\\\\n|[0.0,0.0,0.543840...|       1.0|[0.22746125077264...|        1|\\\\n|[0.0,0.0,0.543841...|       1.0|[0.25300403585920...|        0|\\\\n|[0.0,1.0,0.543845...|       1.0|[0.12309867929353...|        1|\\\\n|[0.0,0.0,0.543845...|       1.0|[0.06263953449467...|        1|\\\\n|[0.0,0.0,0.543845...|       1.0|[0.18952662085341...|        0|\\\\n|[0.0,0.0,0.543845...|       1.0|[0.18952662085341...|        1|\\\\n|[0.0,0.0,0.543845...|       0.0|[0.74123091473197...|        0|\\\\n|[0.0,0.0,0.543845...|       1.0|[0.11732984864193...|        1|\\\\n|[0.0,0.0,0.543845...|       1.0|[0.11732984864193...|        1|\\\\n|[0.0,0.0,0.543845...|       1.0|[0.19181164343214...|        1|\\\\n|[0.0,0.0,0.543845...|       1.0|[0.16084064932357...|        1|\\\\n|[0.0,1.0,0.543845...|       1.0|[0.15849227936276...|        1|\\\\n|[0.0,0.0,0.543845...|       1.0|[0.08897964361954...|        1|\\\\n|[0.0,0.0,0.543845...|       1.0|[0.08897964361954...|        1|\\\\n|[0.0,0.0,0.543845...|       1.0|[0.30974592654509...|        1|\\\\n|[0.0,1.0,0.543885...|       1.0|[0.09810410067036...|        1|\\\\n|[0.0,1.0,0.543885...|       1.0|[0.13864413912366...|        1|\\\\n|[0.0,1.0,0.543885...|       1.0|[0.13864413912366...|        1|\\\\n|[0.0,1.0,0.543885...|       1.0|[0.10217845526692...|        1|\\\\n|[0.0,1.0,0.543885...|       1.0|[0.10217845526692...|        1|\\\\n|[0.0,1.0,0.543885...|       1.0|[0.11011632093084...|        1|\\\\n|[0.0,1.0,0.543885...|       1.0|[0.11011632093084...|        1|\\\\n|[0.0,0.0,0.543891...|       1.0|[0.30190134568410...|        1|\\\\n|[0.0,0.0,0.543891...|       1.0|[0.23926361608823...|        0|\\\\n|[0.0,1.0,0.543915...|       1.0|[0.08771891178654...|        1|\\\\n|[0.0,0.0,0.543915...|       1.0|[0.28858142872320...|        0|\\\\n|[1.0,1.0,0.543915...|       1.0|[0.22336974704051...|        1|\\\\n|[0.0,1.0,0.543915...|       1.0|[0.11046066323704...|        0|\\\\n|[0.0,0.0,0.543915...|       1.0|[0.40880285548918...|        0|\\\\n|[0.0,0.0,0.543915...|       0.0|[0.50944947121300...|        0|\\\\n|[0.0,1.0,0.543915...|       0.0|[0.61287303976074...|        0|\\\\n|[0.0,0.0,0.543915...|       1.0|[0.29230865907884...|        0|\\\\n|[0.0,0.0,0.543915...|       1.0|[0.29230865907884...|        0|\\\\n|[0.0,1.0,0.543915...|       1.0|[0.17744143883511...|        1|\\\\n|[0.0,1.0,0.543915...|       1.0|[0.31597959634788...|        1|\\\\n|[0.0,1.0,0.543915...|       1.0|[0.20755996266273...|        1|\\\\n|[0.0,0.0,0.543915...|       0.0|[0.52729816655478...|        0|\\\\n|[0.0,1.0,0.543922...|       1.0|[0.16066943831835...|        1|\\\\n|[0.0,1.0,0.543922...|       1.0|[0.16066943831835...|        1|\\\\n|[0.0,0.0,0.543936...|       1.0|[0.24083368695038...|        1|\\\\n|[0.0,1.0,0.544006...|       1.0|[0.11943766836200...|        1|\\\\n|[0.0,0.0,0.544059...|       0.0|[0.61088209245044...|        1|\\\\n|[0.0,0.0,0.544059...|       1.0|[0.29015165778436...|        1|\\\\n|[0.0,1.0,0.544059...|       1.0|[0.16916614364395...|        1|\\\\n|[0.0,1.0,0.544059...|       1.0|[0.16916614364395...|        1|\\\\n|[0.0,1.0,0.544059...|       1.0|[0.02169055136506...|        1|\\\\n|[0.0,1.0,0.544155...|       0.0|[0.73413143741041...|        0|\\\\n|[0.0,1.0,0.544168...|       1.0|[0.18350214853843...|        1|\\\\n|[0.0,0.0,0.544188...|       1.0|[0.19682908412707...|        1|\\\\n|[0.0,1.0,0.544217...|       1.0|[0.12937064236689...|        1|\\\\n|[0.0,0.0,0.544325...|       1.0|[0.30920849592929...|        1|\\\\n|[0.0,1.0,0.544354...|       1.0|[0.13574016841970...|        1|\\\\n|[0.0,0.0,0.544365...|       1.0|[0.08334393207287...|        1|\\\\n|[0.0,1.0,0.544503...|       1.0|[0.26465685243146...|        1|\\\\n|[0.0,0.0,0.544549...|       1.0|[0.23048319702020...|        1|\\\\n|[0.0,1.0,0.544571...|       1.0|[0.09810410067036...|        1|\\\\n|[0.0,1.0,0.544571...|       1.0|[0.09810410067036...|        1|\\\\n|[0.0,1.0,0.544571...|       1.0|[0.09810410067036...|        1|\\\\n|[0.0,0.0,0.544571...|       0.0|[0.53514283294466...|        0|\\\\n|[0.0,1.0,0.544571...|       1.0|[0.13864413912366...|        1|\\\\n|[0.0,1.0,0.544571...|       1.0|[0.10217845526692...|        1|\\\\n|[0.0,1.0,0.544571...|       1.0|[0.10217845526692...|        1|\\\\n|[0.0,1.0,0.544571...|       1.0|[0.11011632093084...|        1|\\\\n|[0.0,1.0,0.544571...|       0.0|[0.69311707484221...|        0|\\\\n|[0.0,0.0,0.544610...|       0.0|[0.56766975804770...|        0|\\\\n|[0.0,0.0,0.544610...|       1.0|[0.34513425757996...|        0|\\\\n|[0.0,1.0,0.544610...|       1.0|[0.44602370803562...|        1|\\\\n|[0.0,1.0,0.544610...|       0.0|[0.71231215010066...|        0|\\\\n|[1.0,0.0,0.544611...|       1.0|[0.36241078967151...|        1|\\\\n|[0.0,1.0,0.544613...|       1.0|[0.33624220340990...|        0|\\\\n|[0.0,1.0,0.544613...|       1.0|[0.44217961182408...|        0|\\\\n|[0.0,0.0,0.544689...|       1.0|[0.44495100883540...|        0|\\\\n|[0.0,0.0,0.544689...|       1.0|[0.22859576941739...|        0|\\\\n|[0.0,1.0,0.544689...|       1.0|[0.35996955593338...|        0|\\\\n|[0.0,1.0,0.544689...|       1.0|[0.17739863517138...|        0|\\\\n|[0.0,1.0,0.544702...|       0.0|[0.50019890943258...|        0|\\\\n|[0.0,0.0,0.544737...|       0.0|[0.55003999304413...|        0|\\\\n|[0.0,1.0,0.544737...|       0.0|[0.50019890943258...|        0|\\\\n|[0.0,1.0,0.544755...|       1.0|[0.22060827319575...|        1|\\\\n|[0.0,0.0,0.544769...|       1.0|[0.22809262080561...|        0|\\\\n|[0.0,1.0,0.544769...|       1.0|[0.24679459782739...|        0|\\\\n|[0.0,0.0,0.544769...|       1.0|[0.28987441954841...|        0|\\\\n|[0.0,1.0,0.544813...|       1.0|[0.12937064236689...|        1|\\\\n|[0.0,1.0,0.544813...|       1.0|[0.12937064236689...|        1|\\\\n|[0.0,0.0,0.544814...|       1.0|[0.31872548172646...|        1|\\\\n+--------------------+----------+--------------------+---------+\\\\nonly showing top 100 rows\\\\n\\\\n\"}]},\"runtimeInfos\":{\"jobUrl\":{\"propertyName\":\"jobUrl\",\"label\":\"SPARK JOB\",\"tooltip\":\"View in Spark web UI\",\"group\":\"spark\",\"values\":[\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=544\"],\"interpreterSettingId\":\"spark\"}},\"apps\":[],\"jobName\":\"paragraph_1570433237010_1351557776\",\"id\":\"20190814-025216_1281079290\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:20:27+0000\",\"dateFinished\":\"2019-10-07T09:20:34+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:208\"},{\"text\":\"%md \\\\n### Confusion Matrix Metrics\\\\nBinary Classifiers are typically evaluated by creating a ***confusion matrix***, which indicates the number of:\\\\n- True Positives\\\\n- True Negatives\\\\n- False Positives\\\\n- False Negatives\\\\n\\\\nFrom these core measures, other evaluation metrics such as ***precision*** and ***recall*** can be calculated.\\\\n\\\\n\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:20:34+0000\",\"config\":{\"tableHide\":false,\"editorSetting\":{\"language\":\"markdown\",\"editOnDblClick\":true,\"completionKey\":\"TAB\",\"completionSupport\":false},\"colWidth\":12,\"editorMode\":\"ace/mode/markdown\",\"fontSize\":9,\"editorHide\":true,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"HTML\",\"data\":\"<div class=\\\\\"markdown-body\\\\\">\\\\n<h3>Confusion Matrix Metrics</h3>\\\\n<p>Binary Classifiers are typically evaluated by creating a <strong><em>confusion matrix</em></strong>, which indicates the number of:<br/>- True Positives<br/>- True Negatives<br/>- False Positives<br/>- False Negatives</p>\\\\n<p>From these core measures, other evaluation metrics such as <strong><em>precision</em></strong> and <strong><em>recall</em></strong> can be calculated.</p>\\\\n</div>\"}]},\"apps\":[],\"jobName\":\"paragraph_1570433237010_427261199\",\"id\":\"20190624-103923_307162351\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:20:34+0000\",\"dateFinished\":\"2019-10-07T09:20:35+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:209\"},{\"text\":\"%spark.pyspark\\\\n\\\\n# Computing the Confusion Matrix\\\\ntp = float(predicted.filter(\\\\\"prediction == 1.0 AND truelabel == 1\\\\\").count())\\\\nfp = float(predicted.filter(\\\\\"prediction == 1.0 AND truelabel == 0\\\\\").count())\\\\ntn = float(predicted.filter(\\\\\"prediction == 0.0 AND truelabel == 0\\\\\").count())\\\\nfn = float(predicted.filter(\\\\\"prediction == 0.0 AND truelabel == 1\\\\\").count())\\\\nmetrics = spark.createDataFrame([\\\\n  (\\\\\"TP\\\\\", tp),\\\\n  (\\\\\"FP\\\\\", fp),\\\\n  (\\\\\"TN\\\\\", tn),\\\\n  (\\\\\"FN\\\\\", fn),\\\\n  (\\\\\"Precision\\\\\", tp / (tp + fp)),\\\\n  (\\\\\"Recall\\\\\", tp / (tp + fn))],[\\\\\"metric\\\\\", \\\\\"value\\\\\"])\\\\nmetrics.show()\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:20:35+0000\",\"config\":{\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"colWidth\":12,\"editorMode\":\"ace/mode/python\",\"fontSize\":9,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"TEXT\",\"data\":\"+---------+------------------+\\\\n|   metric|             value|\\\\n+---------+------------------+\\\\n|       TP|         2103082.0|\\\\n|       FP|          477144.0|\\\\n|       TN|          653072.0|\\\\n|       FN|           95828.0|\\\\n|Precision|0.8150766638271221|\\\\n|   Recall|0.9564202263848907|\\\\n+---------+------------------+\\\\n\\\\n\"}]},\"runtimeInfos\":{\"jobUrl\":{\"propertyName\":\"jobUrl\",\"label\":\"SPARK JOB\",\"tooltip\":\"View in Spark web UI\",\"group\":\"spark\",\"values\":[\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=545\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=546\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=547\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=548\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=549\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=550\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=551\"],\"interpreterSettingId\":\"spark\"}},\"apps\":[],\"jobName\":\"paragraph_1570433237011_373459831\",\"id\":\"20190624-103949_2018659094\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:20:35+0000\",\"dateFinished\":\"2019-10-07T09:20:55+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:210\"},{\"text\":\"%md\\\\n**Result:** *Precision (0.8187202905101071), Recall (0.9560586358048228)*. Althought the Precision (TP/(TP+FP)) is lower - I think it can be neglected as it is more important not to miss the actual jams, which means keep FN low and Recall higher. Since we are prediction traffic jams on the road, for city planning and road planning it is important not to miss the actual traffic congestion.\\\\n\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:20:55+0000\",\"config\":{\"colWidth\":12,\"fontSize\":9,\"enabled\":true,\"results\":{},\"editorSetting\":{\"language\":\"markdown\",\"editOnDblClick\":true,\"completionKey\":\"TAB\",\"completionSupport\":false},\"editorMode\":\"ace/mode/markdown\",\"editorHide\":true,\"tableHide\":false},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"HTML\",\"data\":\"<div class=\\\\\"markdown-body\\\\\">\\\\n<p><strong>Result:</strong> <em>Precision (0.8187202905101071), Recall (0.9560586358048228)</em>. Althought the Precision (TP/(TP+FP)) is lower - I think it can be neglected as it is more important not to miss the actual jams, which means keep FN low and Recall higher. Since we are prediction traffic jams on the road, for city planning and road planning it is important not to miss the actual traffic congestion.</p>\\\\n</div>\"}]},\"apps\":[],\"jobName\":\"paragraph_1570433892717_-48128020\",\"id\":\"20191007-073812_1824319256\",\"dateCreated\":\"2019-10-07T07:38:12+0000\",\"dateStarted\":\"2019-10-07T09:20:55+0000\",\"dateFinished\":\"2019-10-07T09:20:55+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:211\"},{\"text\":\"%md\\\\n\\\\n### Area Under ROC\\\\nFor evaluator we are using **BinaryClassificationEvaluator**, since our problem is 2-class classification.\\\\n\\\\n**Result:** *AUR = 0.77228639197655095*. I think it is a very good AUR. After trying different algorithms, combinations of parameters, Cross Validation, selecting different combination of features and ways to transorm the data - this results appeares to the best.\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:20:55+0000\",\"config\":{\"tableHide\":false,\"editorSetting\":{\"language\":\"markdown\",\"editOnDblClick\":true,\"completionKey\":\"TAB\",\"completionSupport\":false},\"colWidth\":12,\"editorMode\":\"ace/mode/markdown\",\"fontSize\":9,\"editorHide\":true,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"HTML\",\"data\":\"<div class=\\\\\"markdown-body\\\\\">\\\\n<h3>Area Under ROC</h3>\\\\n<p>For evaluator we are using <strong>BinaryClassificationEvaluator</strong>, since our problem is 2-class classification.</p>\\\\n<p><strong>Result:</strong> <em>AUR = 0.77228639197655095</em>. I think it is a very good AUR. After trying different algorithms, combinations of parameters, Cross Validation, selecting different combination of features and ways to transorm the data - this results appeares to the best.</p>\\\\n</div>\"}]},\"apps\":[],\"jobName\":\"paragraph_1570433237011_-1490597786\",\"id\":\"20190624-104004_861797389\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:20:55+0000\",\"dateFinished\":\"2019-10-07T09:20:55+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:212\"},{\"text\":\"%spark.pyspark\\\\n\\\\n#RF: rawPredictionCol=\\\\\"prediction\\\\\", metricName=\\\\\"areaUnderROC\\\\\"\\\\n\\\\nevaluator = BinaryClassificationEvaluator(labelCol=\\\\\"trueLabel\\\\\", rawPredictionCol=\\\\\"prediction\\\\\", metricName=\\\\\"areaUnderROC\\\\\")\\\\naur = evaluator.evaluate(prediction)\\\\nprint (\\\\\"AUR = \\\\\", aur)\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:20:56+0000\",\"config\":{\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"colWidth\":12,\"editorMode\":\"ace/mode/python\",\"fontSize\":9,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"TEXT\",\"data\":\"(\\'AUR = \\', 0.76712479852693)\\\\n\"}]},\"runtimeInfos\":{\"jobUrl\":{\"propertyName\":\"jobUrl\",\"label\":\"SPARK JOB\",\"tooltip\":\"View in Spark web UI\",\"group\":\"spark\",\"values\":[\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=552\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=553\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=554\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=555\"],\"interpreterSettingId\":\"spark\"}},\"apps\":[],\"jobName\":\"paragraph_1570433237011_373179112\",\"id\":\"20190624-104023_1491801877\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:20:56+0000\",\"dateFinished\":\"2019-10-07T09:20:58+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:213\"},{\"text\":\"%md\\\\n\\\\n## Next step\\\\nTrain *Random Forest* model using *Cross Validation* and then compare the result with the above *Train Validation Split*\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:20:58+0000\",\"config\":{\"tableHide\":false,\"editorSetting\":{\"language\":\"markdown\",\"editOnDblClick\":true,\"completionKey\":\"TAB\",\"completionSupport\":false},\"colWidth\":12,\"editorMode\":\"ace/mode/markdown\",\"fontSize\":9,\"editorHide\":true,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"HTML\",\"data\":\"<div class=\\\\\"markdown-body\\\\\">\\\\n<h2>Next step</h2>\\\\n<p>Train <em>Random Forest</em> model using <em>Cross Validation</em> and then compare the result with the above <em>Train Validation Split</em></p>\\\\n</div>\"}]},\"apps\":[],\"jobName\":\"paragraph_1570433237012_-1334432553\",\"id\":\"20190624-104041_453435522\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:20:58+0000\",\"dateFinished\":\"2019-10-07T09:20:58+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:214\"},{\"text\":\"%md\\\\n### Tuning Parameters with Cross Validator\\\\n\\\\n**Cross Validator** is used to evaluate each combination of parameters defined in a **ParameterGrid**, which we already defined early above. We are tuning the parameter **numTrees** - number of trees in the forest and **maxDepth** - the maximum depth of each tree. This step will take some more time to compute. \",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:20:58+0000\",\"config\":{\"tableHide\":false,\"editorSetting\":{\"language\":\"markdown\",\"editOnDblClick\":true,\"completionKey\":\"TAB\",\"completionSupport\":false},\"colWidth\":12,\"editorMode\":\"ace/mode/markdown\",\"fontSize\":9,\"editorHide\":true,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"HTML\",\"data\":\"<div class=\\\\\"markdown-body\\\\\">\\\\n<h3>Tuning Parameters with Cross Validator</h3>\\\\n<p><strong>Cross Validator</strong> is used to evaluate each combination of parameters defined in a <strong>ParameterGrid</strong>, which we already defined early above. We are tuning the parameter <strong>numTrees</strong> - number of trees in the forest and <strong>maxDepth</strong> - the maximum depth of each tree. This step will take some more time to compute.</p>\\\\n</div>\"}]},\"apps\":[],\"jobName\":\"paragraph_1570433237012_-1399732686\",\"id\":\"20190624-104102_1404017125\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:20:58+0000\",\"dateFinished\":\"2019-10-07T09:20:58+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:215\"},{\"text\":\"%spark.pyspark\\\\n# K=2, 3, 5, \\\\n# K= 10 takes too long\\\\ncv = CrossValidator(estimator=rf, evaluator=BinaryClassificationEvaluator(), \\\\\\\\\\\\n                    estimatorParamMaps=paramGrid, numFolds=2)\\\\n\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:20:58+0000\",\"config\":{\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"colWidth\":12,\"editorMode\":\"ace/mode/python\",\"fontSize\":9,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[]},\"apps\":[],\"jobName\":\"paragraph_1570433237012_1278323261\",\"id\":\"20190813-081604_921239690\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:20:58+0000\",\"dateFinished\":\"2019-10-07T09:20:58+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:216\"},{\"text\":\"%spark.pyspark\\\\n\\\\n# the first best model\\\\nCVmodel = cv.fit(transformedTraining)\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:20:58+0000\",\"config\":{\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"colWidth\":12,\"editorMode\":\"ace/mode/python\",\"fontSize\":9,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[]},\"runtimeInfos\":{\"jobUrl\":{\"propertyName\":\"jobUrl\",\"label\":\"SPARK JOB\",\"tooltip\":\"View in Spark web UI\",\"group\":\"spark\",\"values\":[\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=556\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=557\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=558\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=559\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=560\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=561\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=562\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=563\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=564\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=565\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=566\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=567\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=568\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=569\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=570\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=571\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=572\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=573\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=574\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=575\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=576\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=577\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=578\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=579\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=580\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=581\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=582\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=583\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=584\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=585\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=586\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=587\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=588\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=589\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=590\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=591\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=592\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=593\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=594\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=595\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=596\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=597\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=598\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=599\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=600\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=601\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=602\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=603\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=604\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=605\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=606\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=607\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=608\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=609\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=610\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=611\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=612\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=613\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=614\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=615\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=616\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=617\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=618\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=619\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=620\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=621\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=622\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=623\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=624\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=625\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=626\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=627\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=628\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=629\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=630\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=631\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=632\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=633\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=634\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=635\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=636\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=637\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=638\"],\"interpreterSettingId\":\"spark\"}},\"apps\":[],\"jobName\":\"paragraph_1570433237013_-1890795895\",\"id\":\"20190624-104118_1662425804\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:20:58+0000\",\"dateFinished\":\"2019-10-07T09:25:48+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:217\"},{\"text\":\"%md\\\\n## Testing CV Model\\\\n\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:25:48+0000\",\"config\":{\"tableHide\":false,\"editorSetting\":{\"language\":\"markdown\",\"editOnDblClick\":true,\"completionKey\":\"TAB\",\"completionSupport\":false},\"colWidth\":12,\"editorMode\":\"ace/mode/markdown\",\"fontSize\":9,\"editorHide\":true,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"HTML\",\"data\":\"<div class=\\\\\"markdown-body\\\\\">\\\\n<h2>Testing CV Model</h2>\\\\n</div>\"}]},\"apps\":[],\"jobName\":\"paragraph_1570433237013_2133950590\",\"id\":\"20190624-104137_1530430704\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:25:48+0000\",\"dateFinished\":\"2019-10-07T09:25:48+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:218\"},{\"text\":\"%spark.pyspark\\\\n\\\\nCVprediction = CVmodel.transform(transformedTest)\\\\nCVprediction.persist(StorageLevel.MEMORY_AND_DISK)\\\\n\\\\n\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:25:48+0000\",\"config\":{\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"colWidth\":12,\"editorMode\":\"ace/mode/python\",\"fontSize\":9,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"TEXT\",\"data\":\"DataFrame[location_x: double, location_y: double, sin_weekday: double, cos_weekday: double, sin_month: double, cos_month: double, sin_day: double, cos_day: double, sin_hour: double, cos_hour: double, sin_min: double, cos_min: double, sin_sec: double, cos_sec: double, is_rush: int, is_weekend: int, is_holiday: int, level: int, trueLabel: int, numFeatures: vector, normFeatures: vector, catFeatures: vector, idxCatFeatures: vector, features: vector, rawPrediction: vector, probability: vector, prediction: double]\\\\n\"}]},\"apps\":[],\"jobName\":\"paragraph_1570433237013_1212592629\",\"id\":\"20190624-104149_899093183\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:25:48+0000\",\"dateFinished\":\"2019-10-07T09:25:48+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:219\"},{\"text\":\"%spark.pyspark\\\\n\\\\nCVpredicted = CVprediction.select(\\\\\"features\\\\\", \\\\\"prediction\\\\\", \\\\\"probability\\\\\", \\\\\"trueLabel\\\\\")\\\\nCVpredicted.show(100)\\\\n\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:25:48+0000\",\"config\":{\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"colWidth\":12,\"editorMode\":\"ace/mode/python\",\"fontSize\":9,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"TEXT\",\"data\":\"+--------------------+----------+--------------------+---------+\\\\n|            features|prediction|         probability|trueLabel|\\\\n+--------------------+----------+--------------------+---------+\\\\n|[0.0,0.0,0.543711...|       1.0|[0.19682908412707...|        1|\\\\n|[0.0,0.0,0.543714...|       1.0|[0.30372873591197...|        1|\\\\n|[1.0,0.0,0.543719...|       1.0|[0.34441877494528...|        1|\\\\n|[0.0,0.0,0.543749...|       0.0|[0.61656786783814...|        0|\\\\n|[0.0,0.0,0.543766...|       1.0|[0.12154818765592...|        1|\\\\n|[1.0,0.0,0.543767...|       0.0|[0.73743051882772...|        0|\\\\n|[0.0,0.0,0.543767...|       1.0|[0.39105247678967...|        0|\\\\n|[0.0,1.0,0.543772...|       1.0|[0.18081019427021...|        1|\\\\n|[0.0,1.0,0.543778...|       1.0|[0.14616887010758...|        1|\\\\n|[0.0,0.0,0.543778...|       1.0|[0.18019906021068...|        1|\\\\n|[0.0,0.0,0.543780...|       0.0|[0.75315324234508...|        0|\\\\n|[0.0,0.0,0.543780...|       0.0|[0.51572997379587...|        0|\\\\n|[0.0,1.0,0.543790...|       1.0|[0.12918485748462...|        1|\\\\n|[0.0,1.0,0.543835...|       1.0|[0.05990719170346...|        1|\\\\n|[0.0,0.0,0.543840...|       1.0|[0.28695159811187...|        1|\\\\n|[0.0,0.0,0.543840...|       1.0|[0.22746125077264...|        1|\\\\n|[0.0,0.0,0.543841...|       1.0|[0.25300403585920...|        0|\\\\n|[0.0,1.0,0.543845...|       1.0|[0.12309867929353...|        1|\\\\n|[0.0,0.0,0.543845...|       1.0|[0.06263953449467...|        1|\\\\n|[0.0,0.0,0.543845...|       1.0|[0.18952662085341...|        0|\\\\n|[0.0,0.0,0.543845...|       1.0|[0.18952662085341...|        1|\\\\n|[0.0,0.0,0.543845...|       0.0|[0.74123091473197...|        0|\\\\n|[0.0,0.0,0.543845...|       1.0|[0.11732984864193...|        1|\\\\n|[0.0,0.0,0.543845...|       1.0|[0.11732984864193...|        1|\\\\n|[0.0,0.0,0.543845...|       1.0|[0.19181164343214...|        1|\\\\n|[0.0,0.0,0.543845...|       1.0|[0.16084064932357...|        1|\\\\n|[0.0,1.0,0.543845...|       1.0|[0.15849227936276...|        1|\\\\n|[0.0,0.0,0.543845...|       1.0|[0.08897964361954...|        1|\\\\n|[0.0,0.0,0.543845...|       1.0|[0.08897964361954...|        1|\\\\n|[0.0,0.0,0.543845...|       1.0|[0.30974592654509...|        1|\\\\n|[0.0,1.0,0.543885...|       1.0|[0.09810410067036...|        1|\\\\n|[0.0,1.0,0.543885...|       1.0|[0.13864413912366...|        1|\\\\n|[0.0,1.0,0.543885...|       1.0|[0.13864413912366...|        1|\\\\n|[0.0,1.0,0.543885...|       1.0|[0.10217845526692...|        1|\\\\n|[0.0,1.0,0.543885...|       1.0|[0.10217845526692...|        1|\\\\n|[0.0,1.0,0.543885...|       1.0|[0.11011632093084...|        1|\\\\n|[0.0,1.0,0.543885...|       1.0|[0.11011632093084...|        1|\\\\n|[0.0,0.0,0.543891...|       1.0|[0.30190134568410...|        1|\\\\n|[0.0,0.0,0.543891...|       1.0|[0.23926361608823...|        0|\\\\n|[0.0,1.0,0.543915...|       1.0|[0.08771891178654...|        1|\\\\n|[0.0,0.0,0.543915...|       1.0|[0.28858142872320...|        0|\\\\n|[1.0,1.0,0.543915...|       1.0|[0.22336974704051...|        1|\\\\n|[0.0,1.0,0.543915...|       1.0|[0.11046066323704...|        0|\\\\n|[0.0,0.0,0.543915...|       1.0|[0.40880285548918...|        0|\\\\n|[0.0,0.0,0.543915...|       0.0|[0.50944947121300...|        0|\\\\n|[0.0,1.0,0.543915...|       0.0|[0.61287303976074...|        0|\\\\n|[0.0,0.0,0.543915...|       1.0|[0.29230865907884...|        0|\\\\n|[0.0,0.0,0.543915...|       1.0|[0.29230865907884...|        0|\\\\n|[0.0,1.0,0.543915...|       1.0|[0.17744143883511...|        1|\\\\n|[0.0,1.0,0.543915...|       1.0|[0.31597959634788...|        1|\\\\n|[0.0,1.0,0.543915...|       1.0|[0.20755996266273...|        1|\\\\n|[0.0,0.0,0.543915...|       0.0|[0.52729816655478...|        0|\\\\n|[0.0,1.0,0.543922...|       1.0|[0.16066943831835...|        1|\\\\n|[0.0,1.0,0.543922...|       1.0|[0.16066943831835...|        1|\\\\n|[0.0,0.0,0.543936...|       1.0|[0.24083368695038...|        1|\\\\n|[0.0,1.0,0.544006...|       1.0|[0.11943766836200...|        1|\\\\n|[0.0,0.0,0.544059...|       0.0|[0.61088209245044...|        1|\\\\n|[0.0,0.0,0.544059...|       1.0|[0.29015165778436...|        1|\\\\n|[0.0,1.0,0.544059...|       1.0|[0.16916614364395...|        1|\\\\n|[0.0,1.0,0.544059...|       1.0|[0.16916614364395...|        1|\\\\n|[0.0,1.0,0.544059...|       1.0|[0.02169055136506...|        1|\\\\n|[0.0,1.0,0.544155...|       0.0|[0.73413143741041...|        0|\\\\n|[0.0,1.0,0.544168...|       1.0|[0.18350214853843...|        1|\\\\n|[0.0,0.0,0.544188...|       1.0|[0.19682908412707...|        1|\\\\n|[0.0,1.0,0.544217...|       1.0|[0.12937064236689...|        1|\\\\n|[0.0,0.0,0.544325...|       1.0|[0.30920849592929...|        1|\\\\n|[0.0,1.0,0.544354...|       1.0|[0.13574016841970...|        1|\\\\n|[0.0,0.0,0.544365...|       1.0|[0.08334393207287...|        1|\\\\n|[0.0,1.0,0.544503...|       1.0|[0.26465685243146...|        1|\\\\n|[0.0,0.0,0.544549...|       1.0|[0.23048319702020...|        1|\\\\n|[0.0,1.0,0.544571...|       1.0|[0.09810410067036...|        1|\\\\n|[0.0,1.0,0.544571...|       1.0|[0.09810410067036...|        1|\\\\n|[0.0,1.0,0.544571...|       1.0|[0.09810410067036...|        1|\\\\n|[0.0,0.0,0.544571...|       0.0|[0.53514283294466...|        0|\\\\n|[0.0,1.0,0.544571...|       1.0|[0.13864413912366...|        1|\\\\n|[0.0,1.0,0.544571...|       1.0|[0.10217845526692...|        1|\\\\n|[0.0,1.0,0.544571...|       1.0|[0.10217845526692...|        1|\\\\n|[0.0,1.0,0.544571...|       1.0|[0.11011632093084...|        1|\\\\n|[0.0,1.0,0.544571...|       0.0|[0.69311707484221...|        0|\\\\n|[0.0,0.0,0.544610...|       0.0|[0.56766975804770...|        0|\\\\n|[0.0,0.0,0.544610...|       1.0|[0.34513425757996...|        0|\\\\n|[0.0,1.0,0.544610...|       1.0|[0.44602370803562...|        1|\\\\n|[0.0,1.0,0.544610...|       0.0|[0.71231215010066...|        0|\\\\n|[1.0,0.0,0.544611...|       1.0|[0.36241078967151...|        1|\\\\n|[0.0,1.0,0.544613...|       1.0|[0.33624220340990...|        0|\\\\n|[0.0,1.0,0.544613...|       1.0|[0.44217961182408...|        0|\\\\n|[0.0,0.0,0.544689...|       1.0|[0.44495100883540...|        0|\\\\n|[0.0,0.0,0.544689...|       1.0|[0.22859576941739...|        0|\\\\n|[0.0,1.0,0.544689...|       1.0|[0.35996955593338...|        0|\\\\n|[0.0,1.0,0.544689...|       1.0|[0.17739863517138...|        0|\\\\n|[0.0,1.0,0.544702...|       0.0|[0.50019890943258...|        0|\\\\n|[0.0,0.0,0.544737...|       0.0|[0.55003999304413...|        0|\\\\n|[0.0,1.0,0.544737...|       0.0|[0.50019890943258...|        0|\\\\n|[0.0,1.0,0.544755...|       1.0|[0.22060827319575...|        1|\\\\n|[0.0,0.0,0.544769...|       1.0|[0.22809262080561...|        0|\\\\n|[0.0,1.0,0.544769...|       1.0|[0.24679459782739...|        0|\\\\n|[0.0,0.0,0.544769...|       1.0|[0.28987441954841...|        0|\\\\n|[0.0,1.0,0.544813...|       1.0|[0.12937064236689...|        1|\\\\n|[0.0,1.0,0.544813...|       1.0|[0.12937064236689...|        1|\\\\n|[0.0,0.0,0.544814...|       1.0|[0.31872548172646...|        1|\\\\n+--------------------+----------+--------------------+---------+\\\\nonly showing top 100 rows\\\\n\\\\n\"}]},\"runtimeInfos\":{\"jobUrl\":{\"propertyName\":\"jobUrl\",\"label\":\"SPARK JOB\",\"tooltip\":\"View in Spark web UI\",\"group\":\"spark\",\"values\":[\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=639\"],\"interpreterSettingId\":\"spark\"}},\"apps\":[],\"jobName\":\"paragraph_1570433237013_-1242712575\",\"id\":\"20190814-025326_438532277\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:25:48+0000\",\"dateFinished\":\"2019-10-07T09:25:54+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:220\"},{\"text\":\"%md\\\\n\\\\n### Confusion Matrix Metrics for CV model\\\\n\\\\n**Result:** *Precision (0.7820533903415009), Recall (0.8594060598759111)*. Althought the Precision (TP/(TP+FP)) is lower - I think it can be neglected as it is more important not to miss the actual jams, which means keep FN low and Recall higher. Since we are prediction traffic jams on the road, for city planning and road planning it is important not to miss the actual traffic congestion.\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:25:55+0000\",\"config\":{\"tableHide\":false,\"editorSetting\":{\"language\":\"markdown\",\"editOnDblClick\":true,\"completionKey\":\"TAB\",\"completionSupport\":false},\"colWidth\":12,\"editorMode\":\"ace/mode/markdown\",\"fontSize\":9,\"editorHide\":true,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"HTML\",\"data\":\"<div class=\\\\\"markdown-body\\\\\">\\\\n<h3>Confusion Matrix Metrics for CV model</h3>\\\\n<p><strong>Result:</strong> <em>Precision (0.7820533903415009), Recall (0.8594060598759111)</em>. Althought the Precision (TP/(TP+FP)) is lower - I think it can be neglected as it is more important not to miss the actual jams, which means keep FN low and Recall higher. Since we are prediction traffic jams on the road, for city planning and road planning it is important not to miss the actual traffic congestion.</p>\\\\n</div>\"}]},\"apps\":[],\"jobName\":\"paragraph_1570433237014_-1740967628\",\"id\":\"20190624-104215_1543612778\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:25:55+0000\",\"dateFinished\":\"2019-10-07T09:25:55+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:221\"},{\"text\":\"%spark.pyspark\\\\n\\\\n# Computing the Confusion Matrix for CV model\\\\nCVtp = float(CVpredicted.filter(\\\\\"prediction == 1.0 AND truelabel == 1\\\\\").count())\\\\nCVfp = float(CVpredicted.filter(\\\\\"prediction == 1.0 AND truelabel == 0\\\\\").count())\\\\nCVtn = float(CVpredicted.filter(\\\\\"prediction == 0.0 AND truelabel == 0\\\\\").count())\\\\nCVfn = float(CVpredicted.filter(\\\\\"prediction == 0.0 AND truelabel == 1\\\\\").count())\\\\nCVmetrics = spark.createDataFrame([\\\\n  (\\\\\"TP\\\\\", CVtp),\\\\n  (\\\\\"FP\\\\\", CVfp),\\\\n  (\\\\\"TN\\\\\", CVtn),\\\\n  (\\\\\"FN\\\\\", CVfn),\\\\n  (\\\\\"Precision\\\\\", CVtp / (CVtp + CVfp)),\\\\n  (\\\\\"Recall\\\\\", CVtp / (CVtp + CVfn))],[\\\\\"metric\\\\\", \\\\\"value\\\\\"])\\\\nCVmetrics.show()\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:25:55+0000\",\"config\":{\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"colWidth\":12,\"editorMode\":\"ace/mode/python\",\"fontSize\":9,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"TEXT\",\"data\":\"+---------+------------------+\\\\n|   metric|             value|\\\\n+---------+------------------+\\\\n|       TP|         2103082.0|\\\\n|       FP|          477144.0|\\\\n|       TN|          653072.0|\\\\n|       FN|           95828.0|\\\\n|Precision|0.8150766638271221|\\\\n|   Recall|0.9564202263848907|\\\\n+---------+------------------+\\\\n\\\\n\"}]},\"runtimeInfos\":{\"jobUrl\":{\"propertyName\":\"jobUrl\",\"label\":\"SPARK JOB\",\"tooltip\":\"View in Spark web UI\",\"group\":\"spark\",\"values\":[\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=640\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=641\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=642\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=643\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=644\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=645\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=646\"],\"interpreterSettingId\":\"spark\"}},\"apps\":[],\"jobName\":\"paragraph_1570433237014_-1229073592\",\"id\":\"20190624-104236_1452363516\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:25:55+0000\",\"dateFinished\":\"2019-10-07T09:26:53+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:222\"},{\"text\":\"%md\\\\n### Area Under ROC\\\\nFor evaluator we are using **BinaryClassificationEvaluator**, since our problem is 2-class classification.\\\\n\\\\n**Result:** *AUR for CV = 0.6952137636015555*. I think it is a very good AUR. After trying different algorithms, combinations of parameters, Cross Validation, selecting different combination of features and ways to transorm the data - this results appeares to the best.\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:26:53+0000\",\"config\":{\"tableHide\":false,\"editorSetting\":{\"language\":\"markdown\",\"editOnDblClick\":true,\"completionKey\":\"TAB\",\"completionSupport\":false},\"colWidth\":12,\"editorMode\":\"ace/mode/markdown\",\"fontSize\":9,\"editorHide\":true,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"HTML\",\"data\":\"<div class=\\\\\"markdown-body\\\\\">\\\\n<h3>Area Under ROC</h3>\\\\n<p>For evaluator we are using <strong>BinaryClassificationEvaluator</strong>, since our problem is 2-class classification.</p>\\\\n<p><strong>Result:</strong> <em>AUR for CV = 0.6952137636015555</em>. I think it is a very good AUR. After trying different algorithms, combinations of parameters, Cross Validation, selecting different combination of features and ways to transorm the data - this results appeares to the best.</p>\\\\n</div>\"}]},\"apps\":[],\"jobName\":\"paragraph_1570433237014_-71481579\",\"id\":\"20190624-104253_726714197\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:26:53+0000\",\"dateFinished\":\"2019-10-07T09:26:53+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:223\"},{\"text\":\"%spark.pyspark\\\\n\\\\n#RF: rawPredictionCol=\\\\\"prediction\\\\\", metricName=\\\\\"areaUnderROC\\\\\"\\\\n\\\\nCVevaluator = BinaryClassificationEvaluator(labelCol=\\\\\"trueLabel\\\\\", rawPredictionCol=\\\\\"prediction\\\\\", metricName=\\\\\"areaUnderROC\\\\\")\\\\n\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:26:53+0000\",\"config\":{\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"colWidth\":12,\"editorMode\":\"ace/mode/python\",\"fontSize\":9,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[]},\"apps\":[],\"jobName\":\"paragraph_1570433237015_1273310300\",\"id\":\"20190624-104325_140269376\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:26:53+0000\",\"dateFinished\":\"2019-10-07T09:26:53+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:224\"},{\"text\":\"%spark.pyspark\\\\nCVaur = evaluator.evaluate(CVprediction)\\\\nprint (\\\\\"AUR = \\\\\", aur)\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:26:53+0000\",\"config\":{\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"colWidth\":12,\"editorMode\":\"ace/mode/python\",\"fontSize\":9,\"results\":{},\"enabled\":true},\"settings\":{\"params\":{},\"forms\":{}},\"results\":{\"code\":\"SUCCESS\",\"msg\":[{\"type\":\"TEXT\",\"data\":\"(\\'AUR = \\', 0.76712479852693)\\\\n\"}]},\"runtimeInfos\":{\"jobUrl\":{\"propertyName\":\"jobUrl\",\"label\":\"SPARK JOB\",\"tooltip\":\"View in Spark web UI\",\"group\":\"spark\",\"values\":[\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=647\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=648\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=649\",\"http://ip-172-31-66-54.ec2.internal:4040/jobs/job?id=650\"],\"interpreterSettingId\":\"spark\"}},\"apps\":[],\"jobName\":\"paragraph_1570433237015_-888569388\",\"id\":\"20190624-104341_599042094\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"dateStarted\":\"2019-10-07T09:26:53+0000\",\"dateFinished\":\"2019-10-07T09:27:23+0000\",\"status\":\"FINISHED\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:225\"},{\"text\":\"%spark.pyspark\\\\n\",\"user\":\"anonymous\",\"dateUpdated\":\"2019-10-07T09:27:23+0000\",\"config\":{\"colWidth\":12,\"fontSize\":9,\"results\":{},\"enabled\":true,\"editorSetting\":{\"language\":\"python\",\"editOnDblClick\":false,\"completionKey\":\"TAB\",\"completionSupport\":true},\"editorMode\":\"ace/mode/python\"},\"settings\":{\"params\":{},\"forms\":{}},\"apps\":[],\"jobName\":\"paragraph_1570433237016_578573479\",\"id\":\"20190815-121237_371144798\",\"dateCreated\":\"2019-10-07T07:27:17+0000\",\"status\":\"FINISHED\",\"errorMessage\":\"\",\"progressUpdateIntervalMs\":500,\"$$hashKey\":\"object:226\"}],\"name\":\"Waze Traffic Test\",\"id\":\"2ENYAVFCT\",\"noteParams\":{},\"noteForms\":{},\"angularObjects\":{\"md:shared_process\":[],\"python:shared_process\":[],\"spark:shared_process\":[]},\"config\":{\"isZeppelinNotebookCronEnable\":false,\"looknfeel\":\"default\",\"personalizedMode\":\"false\"},\"info\":{}}'}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX8TQit5xbYU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b18935a-ec46-483a-d2c4-566009bc39c0"
      },
      "source": [
        "#jsonDF = spark.read.json('example1.json')\n",
        "jsonDF = spark.read.json('Waze Traffic Test (3).json') #example1.json')\n",
        "jsonDF.printSchema()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- angularObjects: struct (nullable = true)\n",
            " |    |-- md:shared_process: array (nullable = true)\n",
            " |    |    |-- element: string (containsNull = true)\n",
            " |    |-- python:shared_process: array (nullable = true)\n",
            " |    |    |-- element: string (containsNull = true)\n",
            " |    |-- spark:shared_process: array (nullable = true)\n",
            " |    |    |-- element: string (containsNull = true)\n",
            " |-- config: struct (nullable = true)\n",
            " |    |-- isZeppelinNotebookCronEnable: boolean (nullable = true)\n",
            " |    |-- looknfeel: string (nullable = true)\n",
            " |    |-- personalizedMode: string (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- paragraphs: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- $$hashKey: string (nullable = true)\n",
            " |    |    |-- apps: array (nullable = true)\n",
            " |    |    |    |-- element: string (containsNull = true)\n",
            " |    |    |-- config: struct (nullable = true)\n",
            " |    |    |    |-- colWidth: long (nullable = true)\n",
            " |    |    |    |-- editorHide: boolean (nullable = true)\n",
            " |    |    |    |-- editorMode: string (nullable = true)\n",
            " |    |    |    |-- editorSetting: struct (nullable = true)\n",
            " |    |    |    |    |-- completionKey: string (nullable = true)\n",
            " |    |    |    |    |-- completionSupport: boolean (nullable = true)\n",
            " |    |    |    |    |-- editOnDblClick: boolean (nullable = true)\n",
            " |    |    |    |    |-- language: string (nullable = true)\n",
            " |    |    |    |-- enabled: boolean (nullable = true)\n",
            " |    |    |    |-- fontSize: long (nullable = true)\n",
            " |    |    |    |-- tableHide: boolean (nullable = true)\n",
            " |    |    |-- dateCreated: string (nullable = true)\n",
            " |    |    |-- dateFinished: string (nullable = true)\n",
            " |    |    |-- dateStarted: string (nullable = true)\n",
            " |    |    |-- dateUpdated: string (nullable = true)\n",
            " |    |    |-- errorMessage: string (nullable = true)\n",
            " |    |    |-- focus: boolean (nullable = true)\n",
            " |    |    |-- id: string (nullable = true)\n",
            " |    |    |-- jobName: string (nullable = true)\n",
            " |    |    |-- progressUpdateIntervalMs: long (nullable = true)\n",
            " |    |    |-- results: struct (nullable = true)\n",
            " |    |    |    |-- code: string (nullable = true)\n",
            " |    |    |    |-- msg: array (nullable = true)\n",
            " |    |    |    |    |-- element: struct (containsNull = true)\n",
            " |    |    |    |    |    |-- data: string (nullable = true)\n",
            " |    |    |    |    |    |-- type: string (nullable = true)\n",
            " |    |    |-- runtimeInfos: struct (nullable = true)\n",
            " |    |    |    |-- jobUrl: struct (nullable = true)\n",
            " |    |    |    |    |-- group: string (nullable = true)\n",
            " |    |    |    |    |-- interpreterSettingId: string (nullable = true)\n",
            " |    |    |    |    |-- label: string (nullable = true)\n",
            " |    |    |    |    |-- propertyName: string (nullable = true)\n",
            " |    |    |    |    |-- tooltip: string (nullable = true)\n",
            " |    |    |    |    |-- values: array (nullable = true)\n",
            " |    |    |    |    |    |-- element: string (containsNull = true)\n",
            " |    |    |-- status: string (nullable = true)\n",
            " |    |    |-- text: string (nullable = true)\n",
            " |    |    |-- user: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ7cLUwd1AbP"
      },
      "source": [
        "# Saving Text Files\n",
        "\n",
        "Using the USDA_activity_dataset_csv dataset (found on iCollege under Datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9_EjG_h1Q5R"
      },
      "source": [
        "**Only in Google Colab:**\n",
        "\n",
        "Load the USDA file from Disk. \n",
        "\n",
        "NOTEL: Convert it to CSV on Excel first! \n",
        "\n",
        "Note: You might have to run this twice so it works fine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvb7iQbv1Q5S"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9w4g1vh1e8j"
      },
      "source": [
        "**Reading a CSV file into a DataFrame, filter some columns and save it**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuKbsrvUVhse"
      },
      "source": [
        "data = spark.read.csv('USDA_activity_dataset_csv.csv',inferSchema=True, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ICHjQoP1hrz"
      },
      "source": [
        "Filter data by several columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hXyJNFVWjux"
      },
      "source": [
        "dataF=data.select(\"State\",\"County\",\"Median household income\",\"Poverty rate\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39l8vOAA18x9"
      },
      "source": [
        "Save only the filtered Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxlQXKjg11mM"
      },
      "source": [
        "dataF.write.csv(\"USDA_income_poverty.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1cg9iM93qCg"
      },
      "source": [
        "Let's read this new file back into an RDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-iSzucQ3sup"
      },
      "source": [
        "rddL=sc.textFile(\"USDA_income_poverty.csv\")\n",
        "rddL.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l9c1Jdp5O6c"
      },
      "source": [
        "# **Hive Example**\n",
        "\n",
        "Using Hive to create and read a table - Simple Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOcCViaN4cwC"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql import HiveContext\n",
        "sqlContext = HiveContext(sc)\n",
        "test_list = [('A', 25),('B', 20),('C', 25),('D', 18)]\n",
        "rdd = sc.parallelize(test_list)\n",
        "people = rdd.map(lambda x: Row(name=x[0], age=int(x[1])))\n",
        "schemaPeople = sqlContext.createDataFrame(people)\n",
        "# Register it as a temp table\n",
        "sqlContext.registerDataFrameAsTable(schemaPeople, \"test_table\")\n",
        "sqlContext.sql(\"show tables\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWcwTJx_5cYW"
      },
      "source": [
        "Let's query the table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaX5OJaz5eAz"
      },
      "source": [
        "sqlContext.sql(\"Select * from test_table\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIcPB7Mi5-Zu"
      },
      "source": [
        "**Load a JSON file with Hive and use SQL on it**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKD0QGQn6G3H"
      },
      "source": [
        "## Colab code only - DO NOT run outsie of colab\n",
        "from google.colab import files  \n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5J7JUJy16OJF"
      },
      "source": [
        "Let's load example1.json with Hive a do a Select Statement on it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axQ3wgQS6aLM"
      },
      "source": [
        "from pyspark.sql import HiveContext\n",
        "hiveCtx = HiveContext(sc)\n",
        "ex1 = hiveCtx.read.json(\"example1.json\")\n",
        "ex1.registerTempTable(\"ex1\")\n",
        "results = hiveCtx.sql(\"SELECT ename, sal FROM ex1\").show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}